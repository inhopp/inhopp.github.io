---
title:  "[CV] Scale-Invariant Feature Transform (SIFT)" 

categories:
  -  Vision
tags:
  - [CS]

toc: true
toc_sticky: true

date: 2022-10-20
last_modified_at: 2022-10-20
---

<br>

# 📹 Introduction to SIFT

![intro](https://user-images.githubusercontent.com/96368476/196869485-f829988f-e00f-4d64-9867-151ea3e71443.png){: width="60%" height="70%" .align-center}

<br>

![result](https://user-images.githubusercontent.com/96368476/196872462-ccbd8c6d-c272-44ac-996d-4fe595cf16fc.png){: width="60%" height="70%" .align-center}

SIFT는 key point를 찾아 descriptor를 붙이는 단계, descriptor를 이용해 원본 이미지와 변환 이미지의 key point를 매칭하는 단계, 총 두 단계로 이루어져 있다. SIFT로 뽑은 key point의 특징으로는 Scale, Rotation, Illumunation, Noise 에 Invariant하다.

<br>

## Scaling Problem

| Original Image | Zoomed Image |
|:-:|:-:|
|![example1](https://user-images.githubusercontent.com/96368476/196873407-760393ae-9b6a-4a0d-819a-b329a9ace459.png)|![example2](https://user-images.githubusercontent.com/96368476/196873399-23f9a2ac-500d-4d52-ac18-4a676bc7ae0f.png)|

Harris Corner Detector에서 보았듯이 기존 detector는 같은 코너를 검출한다 하더라도 대상의 scale에 따라 다른 detector를 사용해야 했다. 실제 이미지에서 물체의 크기는 카메라로부터 얼마나 떨어져 있냐에 의존한다. 예를 들어 위 그림의 오른쪽 이미지는 왼쪽 이미지의 일부분을 확대한 것에 불과하다. 왼쪽 계단과 오른쪽 계단이 동일한 대상임에도 서로 다른 detector를 사용해야 이들을 탐지할 수 있다는 의미이다. <br>SIFT는 'Gaussian Pyramid'라는 기법으로 이를 멋지게 해결한다. ML의 object detection을 공부할 때 보았던 Spatial Pyramid Pooling 기법이랑 매우 유사한데 SIFT는 무려 2004년에 발표된 논문이다. Vision 분야도 정말 똑똑한 사람들이 많았구나 싶다.


<br>

## Algorithm

1. Detect Scale-Space Extrema
2. Accurate Key Point
3. Assign Orientation
4. Key Point Descriptor
5. Key Matching


<br>


# 📹 Detect Scale-Space Extrema

![pyramid](https://user-images.githubusercontent.com/96368476/196880606-f2b73d29-d348-4845-9946-9db697536915.png){: width="60%" height="70%" .align-center}

먼저 Scale-Invariant 특성을 위해 Gaussian Pyramid 기법을 이용한다. Bolb Detection 에서 봤듯이 특정 크기의 feature를 탐지하기 위해서는 Laplacian of Gaussian(LoG) 또는 Difference of Gaussian(DoG)를 이용해야 한다. 이때 Gaussian의 σ, 다시 말해 kernel scale이 blob scale과 일치해야 blob을 탐지할 수 있다. 따라서 같은 이미지 크기(same octave)에서 σ를 변경해가며 blob을 탐지하고, 이미지 전체를 down-sampling(next octave)한 후 다시 σ를 변경해가며 blob을 탐지해나간다. 결과적으로 임의의 scale에 대한 blob detection을 수행하는 것인데, σ를 아주 미세하게 나누는 것보다 훨씬 효율적인 방식이라 할 수 있다. <br>

![laplacian pyramid](https://user-images.githubusercontent.com/96368476/196890002-8588a98d-0040-4ade-af88-096ca962ff8b.png){: width="60%" height="70%" .align-center}

위 그림은 한 octave 내에서, DoG의 결과를 나타낸 것이다. σ가 작은 경우 작은 blob에 반응하며, σ가 점점 커짐에 따라 큰 blob에 반응하는 것을 볼 수 있다. 이런 방식으로 이미지 전체에서 임의의 크기에 대한 bolb을 먼저 탐지하게 된다.

<br>

## Find Candidate Key Points

![find_extrema](https://user-images.githubusercontent.com/96368476/196891295-8e1eccaa-1d4c-42e2-997f-9533c3bb9a3b.png){: width="60%" height="70%" .align-center}

단순히 Bolb Detector에 반응한다고 해서 blob으로 잡아버리면 수많은 중복 탐지 현상이 나타날 것이다. 따라서 Non-max suppression과 같이 중복된 탐지를 제거해야 하는데 Gaussian Pyramid의 경우 DoG의 상위 층과 하위 층까지 모두 합쳐(3차원 cube 형태) 주변 26개의 pixel보다 큰 경우에만 blob으로 처리한다. 이 과정 이후 살아남은 blob들을 key point 후보들로 간주하여 다음 스텝으로 넘긴다.


<br>


# 📹 Accurate Key Point

위 과정에서 구한 key point의 후보들은 local maximum 기준이다. 이제 여기에서 두 가지 경우를 제외할 것이다. 첫째, local maximum 값이 특정 값(threshold)보다 작은 경우. 둘째, 해당 key point가 feature로서의 가치가 적은 edge인 경우.

<br>

## Remove under threshold vaule

이 지점에서 우리가 알고 싶은 것은 local maximun의 실제 값이다. 하지만 주어진 DoG 값은 pixel별로 주어진 discrete한 값들이다. 따라서 2차 Taylor 전개로 interpolation함으로서 연속 함수로 만들어 주고 이 함수에서의 maximun 값을 기준으로 사용한다.

$$ D(X) = D + \frac{\partial D^{T}}{\partial X} X + \frac{1}{2}X^{T}\frac{\partial ^{2}D}{\partial X^{2}}X $$

$$ \hat{X} = -\frac{\partial ^{2} D^{-1}}{\partial X^{2}} \frac{\partial D}{\partial X} $$

$$ D(\hat{X}) = D + \frac{1}{2}\frac{\partial D^{T}}{\partial \hat(X)} $$

$$ if \; \left| D(\hat{X}) \right| < 0.03 \to \textit{Discard key} $$

- Taylor 전개를 통해 DoG 표현
- D를 미분해서 0 나오는 x_hat 찾기
- x_hat 대입해서 최댓값 구하기
- 최댓값이 0.03보다 작으면 key point 후보에서 제외

<br>

## Remove Edge Response

Edge 제거는 이전에 포스팅한 Harris Corner Detector를 이용하면 된다. 해당 key point의 R score가 적당한 threshold보다 작은 경우 key point 후보에서 제거한다.

<br>

## Result

![keypoint](https://user-images.githubusercontent.com/96368476/196965654-d138f2f3-edc7-49ed-b3ad-90ec2db94888.png){: width="70%" height="80%" .align-center}

- (a) : Original Image
- (b) : Original image에서 찾은 key point 후보 (832개)
- (c) : 최댓값이 threshold보다 작은 key point 후보 제거 (729개)
- (d) : Harris Detector로 찾은 Edge key point 제거 (536개)



<br>




[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}