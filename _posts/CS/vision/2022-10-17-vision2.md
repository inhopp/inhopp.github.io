---
title:  "[CV] Bolb Detection (DoG & LoG)" 

categories:
  -  Vision
tags:
  - [CS]

toc: true
toc_sticky: true

date: 2022-10-17
last_modified_at: 2022-10-17
---

**Main Reference: <br>- [Scale-Invariant-Feature Transform(SIFT) - 한국과학기술원 전기 및 전자공학과 최성필](https://trekhleb.dev/blog/2021/content-aware-image-resizing-in-javascript/)**
{: .notice--warning}


<br>

# 📹 Convolution vs Correlation

먼저 vision 분야에서 계속 등장하는 convolution의 개념에 대해 간단히 살펴보자. ML을 공부하면서 막연하게 알고 있던 개념인데 꽤나 중요한 수학적 의미를 가지고 있었다. Correlation과 비교를 통해 이를 알아보자.

$$\mathbf{Convolution  \quad  G = H \star F }$$

$$\textbf{G}\begin{bmatrix}i, j\end{bmatrix}= \sum_{u=-k}^{k}\sum_{v=-k}^{k}H[u,v]F[i-u, j-v]$$

<br>

$$\mathbf{Correlation  \quad  G = H \otimes F }$$

$$\textbf{G}\begin{bmatrix}i, j\end{bmatrix}= \sum_{u=-k}^{k}\sum_{v=-k}^{k}H[u,v]F[i+u, j+v]$$

<br>

![conv](https://user-images.githubusercontent.com/96368476/196095317-327b3280-26c3-4f7d-b68c-6dfd293c3c90.png){: width="40%" height="50%" .align-center}

쉽게 이야기하면 convolution은 filter가 뒤집어진 상태로 지나간다고 보면 된다(2D의 경우 위아래로 뒤집고, 양옆으로 뒤집고). 이게 왜 중요한가 싶지만 Convolution은 Correlation에서 성립하지 않는 교환법칙, 결합법칙이 보장된다. 


<br>


# 📹 Edge Detection

간단한 Edge Detection 과정을 생각해보자. 직관적으로 Edge란 기울기가 급격하게 변하는 부분을 찾으면 될 것 같다. 즉, graient의 크기가 커지는 지점일 것이다. 간단하지만 합리적이로 보이는 이 방법이 잘 작동할까? 아쉽지만 그렇지 않다.

![noise](https://user-images.githubusercontent.com/96368476/196155065-04956a71-0de1-406b-b126-aaf74e7b411d.png){: width="60%" height="70%" .align-center}

실제 데이터에서는 수많은 노이즈가 존재하기 때문에 edge 부분에서만 값이 커지지는 않는다. 따라서 우선 Gaussian blurring(smoothing)을 통해 노이즈를 제거한 뒤 gradient를 구해야 한다. 아래 그림과 같이 주어진 신호를 gaussian filter와 convolution한 후에 미분을 취하게 되면 edge를 찾을 수 있다.

![blurring](https://user-images.githubusercontent.com/96368476/196156057-44ecb7ac-75b7-4dd2-8f1b-39fbb54002f7.png){: width="60%" height="70%" .align-center}

<br>

$$\frac{d}{dx}\begin{pmatrix}f(x)\star g(x)\end{pmatrix} = \begin{pmatrix}\frac{d}{dx}f(x)\end{pmatrix}\star g(x)$$

Convolution은 위와 같은 특이한 미분 법을 가지고 있다. 따라서 gaussian filter와 f를 convolution하고 미분을 취하는 것과, gaussian filter의 미분과 f를 convolution한 결과가 같다. 즉, gaussian을 미리 미분해놓으면 2번 연산할 일을 1번만 연산하고도 구할 수 있다는 의미이다(convolution&derivative -> convolution).

<br>

![smoothing](https://user-images.githubusercontent.com/96368476/196164287-7845fc18-76f9-40aa-a90d-a932de515b37.png)


<br>

# 📹 Sharpening Filter

Blurring을 이용한 재밌는 응용이 있다. 데이터에는 항상 노이즈가 끼어 있다. 다른 말로 하면 데이터는 디테일 파트와 노이즈 파트로 나눌 수 있다. 만약 원본 데이터에 노이즈가 제거된 디테일 파트를 더해준다면 어떤 일이 일어날까? 정답은 디테일 부분이 강조된다. 이를 **Sharpening** 기법이라 한다.

| Filter | Sharpening |
|:-:|:-:|
|![sharpening filter](https://user-images.githubusercontent.com/96368476/196190380-f3d31c10-617c-440d-b1d5-db22bb6721b0.png)|![sharpening](https://user-images.githubusercontent.com/96368476/196190389-63718e1a-fa46-4ff7-a51d-d774959148bb.png)|



<br>

# 📹 Blob Detection

<br>

## Laplacian of Gaussian (LoG)

| Laplacian of Gaussian | Blob Scale |
|:-:|:-:|
|![LoG](https://user-images.githubusercontent.com/96368476/196193254-36509e9e-9265-46c3-bb37-7dd31261cfec.png)|![blob_scale](https://user-images.githubusercontent.com/96368476/196194712-38e11656-6d0a-484c-8cc9-ebccaae94a6d.png)|

Blob이란 이미지 내에서 주변에 비해 특히 밝아지거나 어두워지는 부분을 말한다. 주로 feature의 크기를 결정하는데 사용된다(SIFT에서 다시 등장). 보통 Blob을 탐지할 때에는 가우시안 함수를 두 번 미분한, Laplacian of Gaussian (LoG) Filter를 이용한다. 가우시안은 어쩜 이리 만능일까.. 이때 Blob Detection에서 주의할 지점이 있다. Blob scale에 따라 가우시안의 σ 값을 조절해 주어야 한다. 즉, 큰 blob의 경우 σ가 큰 뚱뚱한 가우시안을 사용해야 한다. 번거롭지만 탐지한 feature의 scale을 결정하기 위해서는 σ를 변경해가며 blob detection을 수행해야 한다.

<br>

## Difference of Gaussian (DoG)

LoG 기법은 blob을 기가 막히게 찾지만 단점이 존재한다. 바로 Laplacian of Gaussian을 σ를 바꿔가며 convolution 해야한다는 것이다. 다 좋은데 너무 비싸다. 그런데 이를 기가 막히게 우회하는 방법, Difference of Gaussian (DoG) 기법이 존재한다. 

<br>

| Graph | LoG vs DoG |
|:-:|:-:|
|![graph-dog](https://user-images.githubusercontent.com/96368476/196207112-7c329bc5-2700-4384-9068-83fb832db610.png){: width="70%" height="80%" .align-center}|![DoG](https://user-images.githubusercontent.com/96368476/196207113-4688a320-5de0-4344-a547-f94dcdc20a32.png)|


Difference of Gaussian은 말 그대로 두 gaussian의 차를 이용하여 Laplacian을 계산하겠다는 것이다. 왼쪽 그림의 빨간 그래프는 Laplcian of Gaussian이다. 그래프에서 두꺼운 가우시안에서 얇은 가우시안을 빼면 빨간 그래프 모양이 나온다는 것을 유추할 수 있다. (이런 내용을 어떻게 발견했을까?) 즉, 우리는 Gaussian의 복잡한 laplacian을 계산하지 않고도 laplacian 값을 얻을 수 있다는 것이다! 이 내용이 바로  SIFT에서 등장할 Gaussian Pyramid의 핵심 개념이다.


<br>



[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}