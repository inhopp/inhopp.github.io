---
title:  "[TF_Tutorial] MoveNet - Pose Detection" 

categories:
  -  TF_Tutorial
tags:
  - [ML, tutorial]

toc: true
toc_sticky: true

date: 2022-02-21
last_modified_at: 2022-02-21
---

**Main Reference: <br>- [TensorFlow Blog : MoveNet](https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html) <br>- [An Overview of Human Pose Estimation with Deep Learning](https://beyondminds.ai/blog/an-overview-of-human-pose-estimation-with-deep-learning/#:~:text=Multi%2DPerson%20Pose%20Estimation&text=This%20method%20is%20known%20as,as%20the%20bottom%2Dup%20approach)** 
{: .notice--warning}


<br>


# 🚤 Pose Detection 이란?

| Input | Pose Detection |
|:-:|:-:|
|![dance_input](https://user-images.githubusercontent.com/96368476/154976986-d4346ce1-3934-4c30-9dda-d343364180ee.gif)|![dance](https://user-images.githubusercontent.com/96368476/154977022-98f057eb-b35a-4bc2-9071-4e76af39e11d.gif)|

<br>

Pose Detection은 특정 신체 부위(어깨, 팔꿈치, 손목, 등등..)의 위치를 추정하여 사람의 포즈를 탐지하는 작업이다. MoveNet은 먼저 17개의 keypoints를 찾고, keypoints로 edge를 만들어 포즈를 추정한다. <br> **[코, 눈, 귀, 어깨, 팔꿈치, 손목, 엉덩이, 무릎, 발목]** - 코를 제외한 나머지 부위는 왼쪽, 오른쪽 x2

<br>

## MoveNet

Pose Detection은 지난 몇 년 많은 발전을 이루었지만, 응용 단계까지 나아가지 못했다. 모델의 퍼포먼스(정확도&속도)가 만족스럽지 않았기 때문이다. MoveNet 모델은 이전 모델인 PoseNet의 성능을 능가하며 특히 속도 측면에서 엄청난 최적화를 이루었다(Ultra fast model이라고 소개한다). MoveNet은 두 가지 버전이 존재하는데, 속도에 중점을 둔 MoveNet_Lightning 버전의 경우 모바일 환경에서도 real-time 연산이 가능하다.

- **MoveNet_Lightning** : Thunder모델에 비해 정확도는 떨어지지만, 초당 50프레임 이상 연산이 가능하고 내장 그래픽카드를 쓰는 노트북이나 모바일 환경에서도 좋은 성능을 보인다.
- **MoveNet_Thunder** : Lightning 모델에 비해 크고 느리지만, 높은 정확도를 보여준다. (그럼에도 초당 30프레임 이상 처리가 가능하다.)


<br>


# 🚤 Model Architecture

![2](https://user-images.githubusercontent.com/96368476/154998888-6caa01f0-5690-455e-a19c-382e5ac60713.png){: width="70%" height="80%" .align-center}

여느 detector 모델과 마찬가지로 MoveNet 역시 feature extraction + prediction (2 stage) 구조를 이루고 있다. 이때 feature extractor는 FPN(feature pyramid network)이 부착된 MobileNetV2를 사용하고, prediction은 4개의 파트로 구성되어있다.

<br>

## Feature Pyramid Network란?

![3](https://user-images.githubusercontent.com/96368476/154999995-32545be5-1ec3-422f-8dac-7751482bbb13.png){: .align-center}

Classification을 위한 단순한 CNN을 생각해보자. 이때 convolition은 object 위치에 크게 상관하지 않는다. 즉, 고양이가 사진 왼쪽에 있든 오른쪽에 있든 CNN 모델은 고양이를 output으로 뱉을 것이다.

<br>

![4](https://user-images.githubusercontent.com/96368476/155002334-51faba53-560e-4bd4-9c3c-52621c5949af.png){: width="60%" height="70%" .align-center}

따라서 object의 위치에 민감한 경우 convolution이 다 끝나고 마지막에 나오는 feature vector 뿐만이 아니라 convolution 중간중간의 feature map 들도 활용할 수 있다. 이러한 방법을 feature pyramid network라 한다.


<br>


## Prediction

![5](https://user-images.githubusercontent.com/96368476/155003833-9c6bec5a-aff4-4849-8a6f-b7e795d3b530.png){: .align-center}

MoveNet의 Prediction head는 4 파트로 구성되어 있다. 각각의 예측은 다음 sequence 작업을 따른다.

- step1 : Person center heat map
  - 이미지에 존재하는 사람들의 중심을 식별한다. 이 중 점수가 가장 높은 위치가 선택된다.
- step2 : Keypoint regression field
  - 선택된 중심 픽셀을 기준으로 keypoints 의 초기값을 생성한다. 이는 rough한 값으로 정확도가 많이 떨어진다.
- step3 : Person keypoint heatmap
  - step2의 초기값으로부터 거리에 반비례하는 가중치를 이용해 heatmap을 만든다 (각 keypoint 마다)
- step4 : 2D per-keypoint offset field
  - fine tuning


<br>


## 상향식 추정 vs 하향식 추정

> 제가 이해하기에 이 부분은 위 내용과 상충합니다. <br>사람의 중심(Person center heat map)을 찾은 후 해당 픽셀을 기준으로 keypoints를 찾으니 top-down 방식이라고 생각됩니다. <br>TensorFlow 블로그에 있는 내용이라 일단 작성했지만, 저는 아직 이해하지 못했습니다.

![1](https://user-images.githubusercontent.com/96368476/154995885-3f6fb803-4a76-40d4-92df-c97921589ecf.png){: width="60%" height="70%" .align-center}

- top-down (하향식) : person detect 먼저 → Pose 추정
- bottom-up (상향식) : keypoints detect → grouping

일반적으로 사람을 먼저 탐지하고 그 안에서 포즈를 추정하는 하향식 방법이 더 간단하다. 하지만 MoveNet의 경우 상향식 방법을 이용한 SinglePose Detector(한 사람만 탐지) 이다.
굳이 이런 방식을 택한 이유는 비디오에서의 성능을 높이기 위해서이다. 간단히 말해 이전 프레임에서 포즈가 감지된 영역을 참고하여 현재 프레임 포즈를 예측하는 방식이다.





<br>


# 🚤 Datasets

모델 Training에는 COCO Dataset과 Google 내부에 존재하는 Active Dataset 을 이용했다. COCO daset의 경우 object detection, segmantation 등 일반적인 Computer Vision 작업을 위해 만들어진 데이터이다. 따라서 구글에서는 Pose Detection만을 위한 Active Dataset을 만들어 훈련시켰다. Active Dataset은 유튜브에 존재하는 영상 중 요가, 운동, 춤 등을 캡쳐한 dataset 이다.

<br>


![7](https://user-images.githubusercontent.com/96368476/155008114-394facf4-820a-4262-8f16-b051765e1c89.png){: width="60%" height="70%" .align-center}


<br>


# 🚤 Performance

## Accuracy

![8](https://user-images.githubusercontent.com/96368476/155009481-987de0cc-f533-4092-a401-a2c2d87817a1.png){: width="80%" height="90%" .align-center}

- 임의의 이미지인 COCO Dataset에서는 성능이 조금 떨어지지만 Active Dataset에서는 상당히 높은 mAP를 보여준다.

<br>

## Speed

![9](https://user-images.githubusercontent.com/96368476/155009484-2f1a3df6-6a40-4d80-a872-5d2b971848a7.png)

- 내장그래픽만으로도 상당히 빠른 속도를 보여준다.


<br>


# 🚤 실습 코드

**[TensorFlow Tutoral Code](https://www.tensorflow.org/hub/tutorials/movenet) : real-time 데모 존재**
{: .notice--primary}

참고) 카메라로부터 3피트 ~ 6피트가 가장 좋다고 한다.


## Lightning을 이용한 real-time detection

> 결과적으로 colab에서 real-time test는 실패했다.<br>colab이 virtural machine 환경이라 노트북 카메라에 접근(cv2.VideoCaputre(0))을 못하더라.<br>js 이용해서 카메라를 키는 방법은 찾았지만 real-time + 프레임별로 쪼개서 detecting 하는데 실패했다. colab 쓰면서 처음으로 화났다<br>아무튼 real-time은 실패하고 녹화 영상으로 대체.

**[Tutoral Code_lightning](https://github.com/inhopp/ML_code/blob/main/MoveNet_lightning.ipynb)**
{: .notice--primary}

![1111](https://user-images.githubusercontent.com/96368476/155013161-2922854a-18c1-4ff7-81f6-50eb7ac42230.gif){: width="70%" height="80%" .align-center}

<br>

## Thunder를 이용한 뮤직비디오 영상 detection

**[Tutoral Code_thunder](https://github.com/inhopp/ML_code/blob/main/MoveNet_thunder.ipynb)**
{: .notice--primary}

| Input | Pose Detection |
|:-:|:-:|
|![1233](https://user-images.githubusercontent.com/96368476/155010552-b08a59b2-6ff5-4095-b451-7bf40200c1ab.gif)|![1234](https://user-images.githubusercontent.com/96368476/155010726-62f8e52a-d415-449f-a0bb-738172eba350.gif)|


<br>
<br>

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}