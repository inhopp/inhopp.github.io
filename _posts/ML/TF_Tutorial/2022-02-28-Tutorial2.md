---
title:  "[TF_Tutorial] On-device Real-time Hand Tracking" 

categories:
  -  TF_Tutorial
tags:
  - [ML, tutorial]

toc: true
toc_sticky: true

date: 2022-02-28
last_modified_at: 2022-02-28
---

**Main Reference: <br>- [TensorFlow Blog : 3D Hand Pose with MediaPipe and TensorFlow.js](https://blog.tensorflow.org/2021/11/3D-handpose.html) <br>- [Google AI Blog : On-Device, Real-Time Hand Tracking with MediaPipe](https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html) <br>- [MediaPipe - Hands](https://google.github.io/mediapipe/solutions/hands.html)** 
{: .notice--warning}



**Paper: <br>- [MediaPipe Hands: On-device Real-time Hand Tracking](https://github.com/inhopp/inhopp/files/8153383/MediaPipe.Hands._.On-device.Real-time.Hand.Tracking.pdf)**
{: .notice--primary}


<br>


# 🚤 Introduction

| Example1 | Example2 |
|:-:|:-:|
|![2](https://user-images.githubusercontent.com/96368476/155987751-499a3007-1343-4962-9f9a-2a5894795474.gif)|![1](https://user-images.githubusercontent.com/96368476/155987674-c5bf968d-ae33-4b37-a887-a069a9a31fb1.gif)|


<br>

![4](https://user-images.githubusercontent.com/96368476/155989618-6187c2e9-0ef9-4a92-87db-5187443d06cc.gif){: width="70%" height="80%" .align-left}

<br>

Hand Tracking 기술은 기계의 제스처 제어, 수화 이해 등 여러 플랫폼에서 중요한 구성 요소가 될 수 있다. 특히 실제 세계의 정보와 디지털 정보가 겹쳐지는 VR/AR 분야에서는 핵심적인 기술이다. 기존의 Hand Tracking 모델들은 손에 장비를 착용하거나, 특수한 카메라를 사용하거나 하는 하드웨어에 의존적인 방식이었지만 우리는 이를 극복하고 다음 성과를 달성하였다.
- 카메라 이외에 추가적인 하드웨어는 필요없다.
- 2개 이상의 손도 탐지할 수 있고, 손의 일부가 가려지더라도 탐지된다.
- 모바일 환경에서 실시간 연산이 가능하다.


<br>


# 🚤 Model Architecture

본 모델은 2 stage Detector 구조를 이루고 있다.

1. Bounding box를 찾는 손바닥 Detector
2. 각 Bounding box(손) 별로 21개의 keypoints 탐지


<br>

## Palm Detector (손바닥 탐지)

![6](https://user-images.githubusercontent.com/96368476/155994537-17b18a91-0aaa-4426-a63e-aa54ae371039.png){: width="40%" height="50%" .align-center}

<br>

Palm Detector는 모바일 환경에서 실시간 얼굴 탐지에 사용되던 BlazeFace 모델과 비슷하게 설계했다. 그림을 보면 convolution 중간중간 feature map을 사용하는 FPN(Feature Pyramid Network)구조와 유사하다는 것을 알 수 있다. 그런데 왜 Hand Detector가 아닌 Palm Detector를 사용했을까? 손의 위치를 찾는 것이 생각보다 쉽지 않기 때문이다. 얼굴의 경우 눈. 코, 입 등 뚜렷한 특징들이 있는 반면 손에서는 그렇다 할 특징이 존재하지 않는다. 따라서 손이 아닌 손바닥(손가락 뺀), Palm Detector를 사용했으며 다음과 같은 특징이 존재한다.
- 손에비해 NMS(Non-Max Suppression)가 잘 먹힌다.
  - 손가락 같이 삐쭉삐쭉하면 Region Proposal이 쉽지 않다.
  - [Object Detection 배경지식](https://inhopp.github.io/paper/Paper2/) 참고
- 손바닥의 bounding box 모양은 정사각형만 고려해도 된다.
  - 일반적인 Region Proposal은 가로:세로 비율이 1:1, 1:2, 2:1인 경우를 고려한다.
- focal loss를 이용한다.
  - object detection에서 사용하는 loss function. (따로 정리해서 블로깅 하겠습니다)



<br>

## Hand Landmark Model

![7](https://user-images.githubusercontent.com/96368476/155994544-d103e93f-ac3e-4c39-8915-557d96b72029.png){: width="70%" height="80%" .align-center}

<br>

- **Input Data**
  - Real world, Synthetic Images : 아래 datasets 목차에서 다룸
  - Hand Presence : '왼손/오른손' 라벨링
  - Handedness : '이미지에 손이 존재하는가?' 라벨링

<br>

## Output : 21 3D LandMarks

![5](https://user-images.githubusercontent.com/96368476/155993065-f1b57661-3601-4d9b-9625-92584ab680a7.png){: width="70%" height="80%" .align-center}

- 여기서 주목할 점은 대략적인 손 모양은 정해져 있다는 것이다.
- 따라서 keypoints만 구하면 3D 손 모양을 비교적 정확하게 렌더링 할 수 있다.
- 이를 통해 상대적인 깊이(relative depth dimension) 차원을 구할 수 있다.
- 2 + 1 = 3D (x, y, relative depth) coordinate

<br>

![8](https://user-images.githubusercontent.com/96368476/156002372-5c077b67-1ad8-4a42-ae8b-e8056afd3b2d.gif){: width="30%" height="40%" .align-left}

- 3D keypoints. 깊을 수록 진한 색으로 표현된다.



<br>

## Implementation in MediaPipe

MideaPipe를 이용한 본 모델의 핵심적인 최적화 방법은 Palm Detector를 필요한 경우에만(매우 드물게) 실행한다는 것이다. 

![9](https://user-images.githubusercontent.com/96368476/156008874-94d3aa17-94d8-4b00-a279-8940794eed57.png){: width="50%" height="60%" .align-center}

현재 프레임에서 LandMarks를 계산할 때 Hand bounding box를 놓쳤는지 여부를 flag로 설정한다(Handedness 값). 만약 손을 놓치지 않았다면 Palm detecting을 실행하지 않고 현재 프레임의 keypoints에서 후속 프레임의 손 위치를 추정한다. 이를 그래프로 표기하면 다음과 같다.

![11](https://user-images.githubusercontent.com/96368476/156010865-42b7884c-8c0e-4e60-845a-7cc0849142e4.png){: width="60%" height="70%" .align-center}

<br>

![10](https://user-images.githubusercontent.com/96368476/156008879-d2069648-5864-4363-94cb-0863412ae967.jpg)

- 왼쪽 그림 : 맨 처음 시행
- Flag_off : 손 놓치지 않음
  - Palm Detecting 실행하지 않고 바로 RandMarks 계산
- Flag_on : 손 놓침
  - Palm Detecting 다시 실행
  - 이후 RandMarks 계산




<br>


# 🚤 Datasets

각 목적에 따라 세 가지 dataset을 이용해 모델을 훈련시켰다.

## In-the-wild dataset

일반적인 이미지 6천 장
- 다양한 배경
- 다양한 손의 크기
- Negative Sample (손 없음)

<br>

## In-house collected gesture dataset

디테일 중심의 이미지 1만 장
- 배경과 손의 크기는 제한
- 손의 각도와 가능한 제스처에 집중

<br>

## Synthetic dataset (합성 이미지)

![12](https://user-images.githubusercontent.com/96368476/156013156-60dccbc3-3b54-4d2e-9ce9-6f5cab570799.png){: width="60%" height="70%" .align-left}

- 그림의 상단 : real-world dataset
- 그림의 하단 : RandMarks를 이용한 손 3D 렌더링 + 다양한 배경 합성

<br>

**Dataset에 따른 성능**

![13](https://user-images.githubusercontent.com/96368476/156015032-41cb343c-8fb0-465d-89f3-3c087dacc1ef.png){: width="40%" height="50%" .align-left}




<br>


# 🚤 Perfomance

![15](https://user-images.githubusercontent.com/96368476/156016386-4ef661c2-189e-4463-b7ce-fe5a14793494.png){: width="60%" height="70%" .align-left}

- 본 모델의 목적은 모바일 환경에서의 실시간 연산
- **<span style="color:red">accuracy vs speed</span>** trade-off 관계로 3 가지 모델 존재
- Light 모델의 경우 아이폰 디바이스에서 1.1ms. 디게 빠름
- Pixel 3가 뭔가 했더니 구글에서 만든 핸드폰 (이게 사회생활..?)

<br>

![14](https://user-images.githubusercontent.com/96368476/156016382-fc39a5df-e188-4824-8575-4ad9f861d2e3.png){: width="60%" height="70%" .align-left}

- 미국 수화 데이터에 대한 성능 평가
- 이전 모델에 비해 성능이 어마어마하게 좋아짐



<br>


# 🚤 실습 코드

**[Tutoral Code](https://github.com/inhopp/ML_code/blob/main/hand_pose.ipynb) <br> [Live Web Demo](https://storage.googleapis.com/tfjs-models/demos/hand-pose-detection/index.html?model=mediapipe_hands)**
{: .notice--primary}

| Input | Hands Pose Detection |
|:-:|:-:|
|![input](https://user-images.githubusercontent.com/96368476/156019811-e67373d6-8068-40a5-8b53-314ed596ebb0.gif)|![output](https://user-images.githubusercontent.com/96368476/156019791-f3dc58ba-8564-4451-9fb7-2d529b1b2201.gif)|

<br>

TensorFlow Blog 글들을 구경하다 보면 기술이 너무 좋아서 깜짝깜짝 놀란다. 이 논문도 2020년에 나왔던데 요즘 vision 분야가 폭발적으로 성장하는 것 같다. 내가 공부하고 싶은 분야에 재밌는 기술들이 쏟아지는 걸 보면 역시 나는 운이 좋나 보다.


<br>
<br>

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}