---
title:  "[논문 리뷰] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks" 

categories:
  -  Paper
tags:
  - [ML, Object Detection]

toc: true
toc_sticky: true

date: 2022-09-17
last_modified_at: 2022-09-17
---

**Paper: <br>- [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://github.com/inhopp/inhopp/files/9591173/Faster.RCNN.pdf)**
{: .notice--primary}


<br>

# 🚀 Abstract

![inference time](https://user-images.githubusercontent.com/96368476/190867158-1f7d3867-45c4-4e47-9a12-40e8fc3fe96d.png){: width="60%" height="70%" .align-center}

R-CNN에서 시작해 Fast R-CNN까지, Detection 모델의 inference time을 비교하면 정말 많이 발전했다. 하지만 여전히 real-time이라 보기는 힘들다. 위 그래프를 보면 inference time의 대부분이 region proposal에 소요된다는 것을 알 수 있다. 기존의 region proposal 방식은 CPU에서 진행되는 Selective Search 방법이다. 우리는 GPU에서 작동하면서 이를 대체할 수 있는 CNN, Region Proposal Network(RPN)를 제안한다. 이를 통해 기존의 방식보다 훨씬 빠르게 영역을 제안하고, Detection 네트워크와 성공적으로 결합시켜 End-to-End 학습도 가능하다. 



<br>


# 🚀 Introduction

기존의 Object Detection 모델은 region proposal 기법과, region based cnn의 결합으로 이루어져 있다. R-CNN의 경우 영역마다 매우 무거운 region based cnn을 수행했는데, 이후 convolution 연산을 먼저 함으로서 수많은 영역들이 feature map을 공유하게 만들었다. 그 결과 연산 비용을 크게 감소시키는데 성공했지만 real-time이라 하기에는 아직 갈 길이 멀었다. 문제의 원인은 이미지당 무려 2초나 소요하는 region proposal method, Selective Search 기법이다. Selective Search method는 DNN기반이 아니기 때문에 기본적으로 느리고, CPU에서 수행해야 한다는 점도 또 하나의 단점이다. <br> 
Region proposal도 DNN 기반으로 gpu에서 돌리면 좋겠다는 생각이 들지만 두 개의 네트워크가 서로 feature들을 공유하고 하나의 네트워크로 결합하는 것은 간단한 일이 아니었을 것이다. 본 논문은 이 부분을 잘 해결했으며 결과적으로 기존의 모델보다 mAP도 상승하고 초당 5 fps 정도까지 속도를 향상시켰다. 정리하자면 본 논문의 핵심은 다음과 같다.

- 첫째, 다양한 크기와 비율을 고려해가며 효율적으로 region proposal을 수행하는 네트워크 RPN을 제안했다.
- 둘째, RPN과 기존의 region-based cnn을 효과적으로 결합시켜 real-time, end-to-end 모델을 완성했다.


<br>


# 🚀 Related Work

외부의 독립적인 모듈을 이용해 region proposal을 하는 방식의 문제인식은 이미 존재했다. OverFeat, MultiBox method와 같이 DNN기반의 region proposal 방식이 몇 제안되었으나, 이들은 region based cnn과의 feature들을 공유하지 못했다.



<br>


# 🚀 Faster R-CNN

![faster-rcnn](https://user-images.githubusercontent.com/96368476/190868489-15045da0-2e26-42f6-9c2d-5738fe9baa28.png){: width="50%" height="60%" .align-center}

Faster R-CNN은 region proposal을 담당하는 RPN, region based cnn을 담당하는 fast r-cnn(exept SS module) 두 모듈로 구성되어 있다. RPN은 마치 Attention 메커니즘처럼 fast r-cnn이 어디를 봐야하는지 알려주는 역할을 하고, 두 모듈은 하나의 통합된 네트워크를 구성한다.


<br>


## Region Proposal Networks (RPN)

![rpn_](https://user-images.githubusercontent.com/96368476/190888727-a05643e9-e6bc-499d-be65-e27f51048ff8.png){: width="60%" height="70%" .align-center}

RPN은 window가 움직이면서 k개의 다양한 영역에 대해 region proposal을 수행한다. 네트워크는 이미지를 입력으로 받으며 2k socres(object가 있는지 여부), 4k coordinates(bounding box coordinates)를 반환한다. <br>


![anchorbox](https://user-images.githubusercontent.com/96368476/190888372-1a5b3b42-d34f-4915-a6a5-448e3ff76ada.jpeg){: width="70%" height="80%" .align-center}

"Anchor"라 불리우는 다양한 영역 후보는 3가지 크기, 3가지 비율 총 9가지로 구성된다. 


<br>


### Translation-Invariant Anchors

![anchor2](https://user-images.githubusercontent.com/96368476/190889002-9331648c-dcdc-49bf-980a-7293da7b1905.png){: width="70%" height="80%" .align-center}

기존 방식들과 달리 anchor기반 region proposal은 translation invariant하다. 즉, 같은 이미지 위에서 object의 위치가 바뀌더라도 RPN은 영향을 받지 않고 잘 찾아낸다. 오른쪽 그림은 anchor들이 이미지의 어떤 영역들을 살피는지를 붉은색 선으로 표현한 것이다. 조금의 모서리 부분을 제외하고 이미지 전체를 골고루 살펴보는 것을 알 수 있다. 이러한 Translation-Invariant한 성질 덕분에 RPN은 overffiting risk도 적고 네트워크도 훨씬 가벼워졌다.


<br>


### Multi-Scale Anchors as Regression References

![anchor3](https://user-images.githubusercontent.com/96368476/190889231-d14d70c0-33fb-4e20-a64b-835815379d08.png){: width="70%" height="80%" .align-center}

Multi-scale prediction에는 세 가지 방법이 존재한다. 첫째, 그림 (a)에서 보여주는 image/feature pyramid 방식이다. 이 방법은 효과적을 수 있으나 시간이나 메모리 측면에서 현실적인 어려움이 존재한다. 둘째, (b)에서 보여주는 pyramid of flters 방식. 이 방법은 image pyramid방식보다 효율적이지만 위치별로 다양한 필터가 필요하다. 따라서 region-based cnn과 feature를 공유할 수 없다. 본 논문에서는 위 두 문제를 모두 해결하는 pyramid of anchors 방식을 사용했다.



<br>



### Loss Function

![rpn](https://user-images.githubusercontent.com/96368476/190888612-c24d1f89-6dde-443e-b91a-4c2f6062bd77.png){: width="70%" height="80%" .align-center}

RPN의 각 anchor box마다 object가 존재하는지를 의미하는 binary class label이 존재한다. 이때 positive label을 주는 경우는 다음 두 가지이다.

- 1) ground truth box와 IoU가 가장 높은 anchor box
- 2) ground truth box와의 IoU가 0.7 이상인 경우

여기서 주목할 점은 ground truth box 하나가 여러개의 anbchor box를 positive label로 선택할 수 있다는 것이다. 보통은 2번 조건으로 결정된 positive label anchor box가 1번 조건과 겹치지만, 2번 조건을 만족하는 anchor box가 없는 경우 1번 조건으로 positive anchor box를 찾게된다. 반대로 ground truth box와의 IoU가 0.3 이하인 경우에만 negative label을 부여하며 negative도 positive도 아닌 anchor에 대해서는 학습을 진행하지 않는다.


<br>


$$ \mathbf{L}(\left\{p_{i} \right\}, \left\{t_{i} \right\}) = \frac{1}{\mathbf{N}_{cl}}\sum_{i}^{}\mathbf{L}_{cls}\left ( p_{i}, p_{i}^{*} \right ) + \lambda \frac{1}{\mathbf{N}_{reg}}\sum_{i}^{}p_{i}^{*}\mathbf{L}_{reg}\left ( t_{i}, t_{i}^{*} \right ) $$

- i : index of anchor in mini-batch
- p_i : anchor 내 object 있을 확률
- p_i* : ground truth label
- t_i : vector associated with predicted bounding box coordinates
- t_i* : vector associated with ground truth box coordinates
- L_cls : log loss
- L_reg : smooth L1 loss
- N_cls : size of mini-batch (default=256)
- N_reg : the number of anchor location
- λ : balancing parameter (default=10)


<br>


### Training RPNs

모든 Region proposal method가 그렇듯이 RPN또한 positive label anchor 보다 negative label anchor가 훨씬 많을 것이다. 따라서 모든 anchor box들을 학습시키면 negative label에 크게 치우친 학습을 진행한다. 이러한 문제를 막기 위해 각 미니배치에서 positive/negative 비율을 1:1로 맞춰가며 anchor box들을 샘플링한다. 


<br>


## Sharing Features for RPNs and Fast R-CNN

![architecture](https://user-images.githubusercontent.com/96368476/190898741-a73d296a-c5c4-4d28-95ed-11e09d5501e6.png){: width="80%" height="90%" .align-center}


지금까지 RPN의 학습 과정에 대해 알아봤다. 그럼 RPN과, Fast R-CNN (region based cnns)이 결합된 Faster R-CNN은 어떻게 작동하고, 또 어떻게 학습할까? RPN과 Fast R-CNN은 앞단의 Convolition 연산을 공유한다. 따라서 효율적으로 두 네트워크를 쌓는데 성공했지만 각 네트워크는 서로 다른 output을 서로 다른 방식으로 학습한다. 이럴 경우 전체 네트워크를 학습하는 몇가지 방법이 있는데 Faster R-CNN이 사용하는 방법은 각 네트워크를 교대로 학습시키는 Alternating Trining 기법을 사용한다.

- 1) Train RPN
- 2) Train Fast R-CNN using result of RPN
- 3) Fine-tune RPN using RPN + Fast R-CNN (freeze VGG)
- 4) Fine-tune Fast R-CNN using RPN + Fast R-CNN (freeze VGG)


<br>


## Implementation Details

![anchor2](https://user-images.githubusercontent.com/96368476/190889002-9331648c-dcdc-49bf-980a-7293da7b1905.png){: width="70%" height="80%" .align-center}

Anchor들이 커버하는 영역을 표시한 그림을 다시 살펴보자. RPN이 각 모서리를 커버하지 않는 이유는 무엇일까? 이미지를 벗어나는 anchor box들은 오히려 훈련을 방해하고 시간만 잡아먹기 때문이다. 따라서 이러한 cross boundary anchor box들은 학습에서 제외시켰다. 1000 x 600 이미지 기준으로 이러한 과정에서 20000개의 anchor boxes가 6000개로 감소한다. 또한 NMS (threshold=0.7)처리 후에는 대략 2000개로 감소한다.




<br>



# 🚀 Experiments

> SS (Selective Search), EB (EdgeBoxes) method보다 RPN이 mAP 더 좋아요

![ablation](https://user-images.githubusercontent.com/96368476/190902832-f84102d6-f473-48ad-91d4-12609eaa1b75.jpg){: width="80%" height="90%" .align-center}

- 앞단의 Convolution layer unshared → mAP 감소
- SS method로 training + RPN으로 Detection → region proposal 300개면 충분하다
- (RPN + ZF) 보다 (RPN + VGG)가 더 좋음 → backbone도 성능에 영향을 미침


<br>

## Performance of VGG-16

![data](https://user-images.githubusercontent.com/96368476/190903868-cbc2da50-ab05-487f-b2ce-f6954bde3001.png){: width="80%" height="90%" .align-center}

- ZF보다 성능이 좋았던 VGG를 이용한 결과
- data를 추가로 학습시키면 성능이 좋아짐

<br>

![latency](https://user-images.githubusercontent.com/96368476/190903939-9c2856b4-21d5-4a7d-9b12-dafde8093b5d.png){: width="80%" height="90%" .align-center}

- SS에서 1510ms이나 걸리던 region poposal 시간이 RPN에서는 10ms 밖에 안걸린다.
- 5 fps 달성!


<br>


## Sensitivie to Hyper-parameters

> Anchor box  setting에 따른 mAP

![anchor ratio](https://user-images.githubusercontent.com/96368476/190904102-4211c962-88a7-41f2-9be8-1b080aac7b50.png){: width="60%" height="70%" .align-center}


<br>

> λ : balancing parameter between classification and box regression

![lambda](https://user-images.githubusercontent.com/96368476/190904244-ace0b1f6-2cf0-43ec-a2fe-1cce6ee992b0.png){: width="50%" height="60%" .align-center}


<br>

## Analysis of Recall-to-IoU

![recall](https://user-images.githubusercontent.com/96368476/190904294-fefdf0a0-d274-4707-acb6-82ddc357ec34.png){: width="80%" height="90%" .align-center}

- 기존의 region proposal 방식은 proposal 갯수를 줄이면 recall이 뚝뚝 떨어짐
- 하지만 RPN은 proposal은 300개로 줄여도 recall 성능이 유지된다
- 따라서 mAP drop 없이 속도를 빠르게 만들 수 있다


<br>


## One-Stage Detection vs Two-State Detection

![two stage](https://user-images.githubusercontent.com/96368476/190904387-034c8dff-a7fc-42c6-9dcf-26bc59e42f3b.png){: width="80%" height="90%" .align-center}

OverFeat model의 경우 pyramid of filters 방식을 이용해 object calssify와 determine location을 동시에 진행하는 one-stage 방식이다. OverFeat System을 참고하여 Faster R-CNN을 one-stage 방식으로 만들어 비교해봤다. 결과는 mAP 크게 감소. <br>

Faster R-CNN에서 one-stage를 시도해봤다는 점에서 조금 놀랐다. 역시 똑똑한 사람들이라 one-stage에 대한 고민은 일찍이 존재했었구나.


<br>





# 🚀 Conclusion

![result](https://user-images.githubusercontent.com/96368476/190904756-c2d2a87c-8d1d-4e63-a1c3-fb2b7e8b425c.png){: width="70%" height="80%" .align-center}


<br>

**[Tutoral Code](https://github.com/inhopp/Faster_RCNN_pytorch)**
{: .notice--primary}

| original_image | Detection |
|:-:|:-:|
| ![orange_91](https://user-images.githubusercontent.com/96368476/192968601-6dae1a8b-e8d6-4df0-a815-9a48d488885e.jpg) | ![detection](https://user-images.githubusercontent.com/96368476/192968604-d6a4fdb6-44a8-4de0-a515-424bc0c29eb4.png) |




<br>
<br>



[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}