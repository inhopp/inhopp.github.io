---
title:  "[논문 리뷰] U-Net: Convolutional Networks for Biomedical Image Segmentation (+ FCN)" 

categories:
  -  Paper
tags:
  - [ML, Segmentation]

toc: true
toc_sticky: true

date: 2022-12-04
last_modified_at: 2022-12-04
---

**Main Reference: <br>- [FCN Paper](https://github.com/inhopp/inhopp/files/10148049/FCN.pdf) <br>- [U-Net Paper](https://github.com/inhopp/inhopp/files/10146872/UNET.pdf) <br>- https://89douner.tistory.com/297**
{: .notice--primary}


<br>

# 🚀 Introduction

2015년에 등장한 Fully Convolutional Networks(FCN)은 CNN기반 segmentation 모델의 초기 버전이다. 오늘 이야기할 U-Net 역시 2015년에 등장했으며 FCN을 개량한 모델이다. U-Net과 FCN의 구조가 매우 비슷하다고 느껴지기 때문에 FCN의 내용은 본 포스팅에서 잠깐 소개하는 것으로 넘어가려 한다. 또 하나 인상적이었던 것은 의료 이미지 데이터에 관한 내용이다. 본 논문을 포함한 많은 논문의 제목에서 'biomedical image 어쩌구' 라는 것을 강조한다. U-Net의 논문을 읽어보며 알게 된 것은 biomedical image이기 때문에 가능한 data augmentation이 존재한다는 것이다. Segmentation이면 그냥 segmentation이지 왜 의료 이미지라는 사족을 달까 싶었는데, 조금이나마 이유를 알 것 같다.



<br>

# 🚀 About Fully Convolutional Networks(FCN)

![fcn](https://user-images.githubusercontent.com/96368476/205480099-1eb11fbd-af2a-445e-b617-0a287ce6898b.png){: width="70%" height="80%" .align-center}

FCN의 구조에는 크게 3가지 특성이 존재한다.
 - Can take arbitrary size as input
 - Upsampling
 - Skip connection

<br>

## Can take arbitrary size as input

만약 누군가 segmentation 작업을 수행해야 한다면 가장 먼저 드는 생각은 '라벨링 된 데이터를 어떻게 구하지?' 일 것이다. Segmentation task의 특성상 픽셀 단위로 라벨링을 해야 하기 때문에 데이터를 만들기가 매우 어렵다. Input size가 조금 다르다고 나만의 작고 소중한 데이터를 버려서 되겠는가. 이러한 문제 때문에 FCN은 기존 CNN모델에서 fully connected layer를 제거했다. CNN에서 fixed input size를 요구하는 범인이 바로 fc layer이기 때문이다. 따라서 fc layer를 1x1 conv layer로 교체했고, 임의의 크기를 가진 데이터를 input으로 받을 수 있게 되었다.

<br>

## Upsampling

| Convolution | Transposed Convolution |
|:-:|:-:|
| ![upsample1](https://user-images.githubusercontent.com/96368476/205480844-38bd17c1-3c5a-4380-8073-db24af74111a.png) | ![upsample2](https://user-images.githubusercontent.com/96368476/205480851-758c23e4-e829-4597-aaed-c29f099ebb31.png) |

Segmenation의 또 하나의 특징은 output data가 단순 클래스가 아니라는 것이다. Input 데이터의 동일한 크기의 size가 요구되며 픽셀 단위로 클래스를 예측해야 한다. 따라서 앞단 conv layer들을 통해 어떤 object들이 존재하는지(context)를 파악하고, input size와 동일한 output size를 얻기 위해 upsampling 과정이 필요하다. Umsampling 과정에는 크게 두가지 방법이 존재한다. 첫째는 non-trainable한 단순 max-pooling, average-pooling의 역연산 과정이고, 둘째는 trainable한 transposed convolution 과정이다. FCN에서는 학습 가능한 transposed convolution 연산을 통해 upsampling을 수행한다. <br>'Pooling의 역연산을 이야기하는데 왜 conv layer가 등장하지?' 싶을 수 있는데 padding이 없는 convolution은 down-sampling이기 때문이다. 이제 transposed-convolution이 무엇인지 이야기해보자. 기존의 convolution 과정을 matrix 곱으로 표현한다면 왼쪽 그림처럼 Ax=y 형태로 표현 가능하다. 이때 y(2x2)를 통해 x(3x3)를 얻고 싶다면, A 대신 A-transpose 형태를 곱해주면 되기 때문이다. (A * x = y  &  A^T * y = x)


<br>

## Skip connection

![fcn_result](https://user-images.githubusercontent.com/96368476/205481717-09285681-d0d1-4dd4-8668-4d478709d6b2.png){: width="50%" height="60%" .align-center}

마지막으로 skip connection 파트이다. FCN은 conv layer를 통해 이미지의 어떤 object들이 존재하는지 알아내고, Upsampling을 통해 object가 어디에 있는지 파악해야 한다. 하지만 앞단 conv layer를 통과하면 object의 위치 정보의 상당 부분을 손실하게 된다. 이러한 문제를 해결하기 위해 conv layer의 중간중간 output을 가져와 upsampling 과정에서 더해주게 된다. 전형적인 spatial pyramid 구조이다. 위 그림은 가장 왼쪽부터 skip connection을 이용하지 않은 경우, 16 stride마다, 8 stride마다 skip connection을 사용한 경우이다. 가장 초기 모델이라 그런지 성능이 좋지 못하다.






<br>


# 🚀 Code

**[U-Net from scratch (pytorch)](https://github.com/inhopp/UNet)**
{: .notice--primary}

![inference](https://user-images.githubusercontent.com/96368476/204140350-ba77117f-8bd0-4c5d-91cb-9ffbfc5df0af.png){: width="70%" height="80%" .align-center}


<br>
<br>



[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}