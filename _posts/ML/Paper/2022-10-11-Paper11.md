---
title:  "[논문 리뷰] You Only Look Once: Unified, Real-Time Object Detection (YOLO)" 

categories:
  -  Paper
tags:
  - [ML, Object Detection]

toc: true
toc_sticky: true

date: 2022-10-11
last_modified_at: 2022-10-11
---

**Paper: <br>- [You Only Look Once: Unified, Real-Time Object Detection (YOLO)](https://github.com/inhopp/inhopp/files/9751618/yolo_v1.pdf)**
{: .notice--primary}


<br>

# 🚀 Abstract

![test](https://user-images.githubusercontent.com/96368476/195012907-6d0ff077-7692-4929-bb14-bdd658a197cd.png){: width="80%" height="90%" .align-center}

지금까지 Detection 작업은 Multi-task problem이었다. Faster R-CNN을 예로 들어보자. 먼저 Region Proposal Networks를 통해 object가 있을 법한 영역을 제안하고 그 위에서 bbox regression & classify 작업을 진행했다. 더욱이 모델 훈련에서는 두 네트워크를 분리하여 fine tuning을 진행해야 하는 등 전체 모델이 복잡한 pipeline을 구성한다. 오늘 소개하는 YOLO 모델은 복잡한 multi-task problem을 하나의 regression problem으로 만들었다. 즉, 기존의 방식처럼 rpn이 제안하는 영역을 기다렸다가 수많은 영역들을 보아야 하는 것이 아니라 전체 이미지를 한 번 보는 것만으로 충분하다는 것이다. 따라서 모델의 inference speed가 많이 빨라졌는데 기본적으로 45fps, 모델을 작게 만든 fast yolo의 경우 155fps를 달성했다. 분명 faster rcnn (5fps) 공부할 때 real-time 모델이라고 배웠는데, 본 논문에서는 5fps가 무슨 real time이냐며 급 나누기를 시전한다. CVPR 2016에서 저자가 직접 real-time detection을 시연하는 영상이 남아있는데 지금 봐도 굉장히 멋지다.


<br>


# 🚀 Introduction

### Faster RCNN Architecture
![architecture](https://user-images.githubusercontent.com/96368476/190898741-a73d296a-c5c4-4d28-95ed-11e09d5501e6.png){: width="70%" height="80%" .align-center}

기존의 Detection 모델, Faster RCNN은 먼저 region proposal을 통해 bounding box를 생성하고 생성된 box에 대해 classification 작업을 수행하는 multi-task 방식이다. 이런 복잡한 방식은 inference 속도가 느리고, 무엇보다 독립적인 네트워크가 따로 훈련하기 때문에 최적화가 매우 까다롭다.

<br>

### YOLO Architecutre

![architecture](https://user-images.githubusercontent.com/96368476/195017645-36272d54-36a3-4496-aa6f-adb0dd530b8d.png){: width="70%" height="80%" .align-center}

반면 YOLO 모델은 하나의 이미지 픽셀로부터 bbox, class porb를 예측하는 single regression problem 구조를  이룬다. 이처럼 원본 이미지 전체를 input으로 받으며, 바로 최적화가 가능하다는 등 여러 장점이 존재한다. YOLO의 장점을 좀 더 자세히 설명하면 다음과 같다.

- YOLO는 complex pipeline이 아닌 single regression problem 구조를 가지기 때문에 매우 빠르다.
    - Experiments 파트에 나오지만, 실질적인 real-time 모델 중 성능이 압도적이다.
- 제안된 특정 영역이 아닌 이미지 전체를 global하게 보고 classify 한다.
    - 즉 객체뿐만 아니라 주변 정보도 같이 학습한다는 의미이다.
    - Faster RCNN에 비해 back-ground error가 절반 이하로 줄어든다.
- YOLO는 조금 더 일반화된 표현을 학습할 수 있다.
    - 위와 비슷한 맥락으로 주변 정보도 함께 학습하기 때문에, 학습 과정에 등장하지 않은 데이터가 들어와도 유연하게 대응한다.


하지만 YOLO도 완벽하지는 않다. 뒤에서 살펴보겠지만 구조적인 문제로 작은 객체를 탐지하는 데 어려움을 겪는다. 또한 속도에 큰 비중을 두어서 다른 detection model들에 비해 mAP가 떨어지는 편이다.



<br>


# 🚀 Model Architecture

![total_architecture](https://user-images.githubusercontent.com/96368476/195088943-442c6ef7-5b5c-4123-97a1-ad0feb29e711.png){: width="60%" height="70%" .align-center}

YOLO의 전체 pipeline을 간단하게 살펴보자. 우선 이미지가 들어오면 feature extract를 담당하는 conv layer를 거친 후 SxS 그리드로 나눈다. 사용한 conv layer는 GoogleNet 모델의 inception 모듈을 사용했다. 다만 inception module을 (1x1 reduction + 3x3 conv) 로 단순화했다.

<br>

![bbox](https://user-images.githubusercontent.com/96368476/195088374-1ceed045-841c-498c-8de0-1bb78e004ff3.png){: width="60%" height="70%" .align-center}

$$ \textbf{C} = \textbf{Pr}\left ( \textbf{Object} \right ) * \textbf{IoU}_{pred}^{truth} $$

이제 전체 이미지 사이즈가 SxS 그리드로 바뀌었다. 각 그리드 셀은 B 개의 bounding box를 예측하는데 하나의 box당 5개의 성분이 존재한다. Bounding box의 위치를 결정하는 w,h,x,y 값들, 그리고 confidence score 이다. Confidence score(C)는 box 안에 객체가 존재하는지, box 모양이 얼마나 정확한지를 판단하는 점수이다. 만약 해당 bbox 안에 객체가 없다면 confidence score의 값은 0이 되어야 한다. 참고로 ground truth bbox는 객체의 중심에 해당하는 그리드 셀에서 생성한 bbox이다.


<br>

![output_tensor](https://user-images.githubusercontent.com/96368476/195088397-ae07b6cc-0502-4574-8feb-d961e2814ba7.png){: width="60%" height="70%" .align-center}


다음으로 각 그리드 셀은 bbox뿐만 아니라 각 class에 대한 probability도 존재한다. Class별 확률은 conditional probability이며, 주의할 점은 bbox별 class probability가 아니라 그리드 셀에 해당하는 class probability이라는 것이다. 여기에서 YOLO의 문제가 발생한다. 각 그리드 셀 당 하나의 class만을 예측하므로 만약 하나의 그리드 셀 안에 복수의 object가 포함되어 있다면 이를 탐지하지 못한다.


<br>


## Training

![training](https://user-images.githubusercontent.com/96368476/195089307-3b1147d4-c6b8-485b-bea1-7c2bf5c7987b.png){: width="70%" height="80%" .align-center}

Feature extraction을 담당하는 앞단 cnn부분은 pretrained weights를 사용했는데 전략이 약간 특이했다. YOLO에서 conv layer는 24개의 layer들로 이루어져 있다. 이 중 앞 20개의 conv layer + global avg pool layer + fc layer로 모델을 만들고 imagenet dataset으로 훈련시켰다. 여기에서 훈련된 conv layer를 yolo 앞단에 가져다 놓고, 24 개 중 남은 4개의 conv layer + 뒷 부분을 Pascal VOC dataset으로 훈련시켰다. <br>

YOLO는 작은 객체들도 놓치지 않기 위해 input image resolutrion을 224x224 에서 448x448로 증가시켰다. Activation function으로는 leaky relu 함수를 사용했고, sum-squared error(SSE) 형식의 Loss function을 정의했다. Loss function을 정의하기에 앞서 문제가 될 수 있는 경우를 생각해보자. 첫 번째로 대부분의 그리드 셀에는 객체가 없을 것이다. 따라서 대부분의 셀에서 confidence score 값은 0일 것이고, 소수의 셀에서 유의미한 값을 가질 것이다. 이러한 불균형은 훈련을 방해한다. 따라서 이를 개선하기 위해 bbox loss의 비중을 높이고, 객체가 없는 그리드 셀의 경우 confidence loss의 비중을 낮췄다 (λ_coord = 5 and λ_noobj = 0.5). 두 번째 문제는 큰 bbox와 작은 bbox가 동일한 가중치를 가진다는 것이다. 작은 bbox의 경우 큰 bbox에 비해 크기에 훨씬 예민할 것이다. 이에 대해서는 squar root를 통해 차이를 완화했다.

<br>

> Loss Function

$$ \lambda _{coord}\sum_{i=0}^{S^{2}}\sum_{j=0}^{B}\mathbb{I}_{ij}^{obj}\begin{bmatrix} (x_{i}-\hat{x}_{i})^{2} + (y_{i}-\hat{y}_{i})^{2}
\end{bmatrix} + \lambda _{coord}\sum_{i=0}^{S^{2}}\sum_{j=0}^{B}\mathbb{I}_{ij}^{obj}\begin{bmatrix} (\sqrt{w_{i}} - \sqrt{\hat{w}_{i}})^{2} + (\sqrt{h_{i}} - \sqrt{\hat{h}_{i}})^{2}
\end{bmatrix} $$

$$ + \sum_{i=0}^{S^{2}}\sum_{j=0}^{B}\mathbb{I}_{ij}^{obj}\begin{pmatrix} C_{i} - \hat{C}_{i}
\end{pmatrix}^{2} + \lambda_{noobj}\sum_{i=0}^{S^{2}}\sum_{j=0}^{B}\mathbb{I}_{ij}^{noobj}\begin{pmatrix} C_{i} - \hat{C}_{i}
\end{pmatrix}^{2} $$

$$ + \sum_{i=0}^{S^{2}}\mathbb{I}_{i}^{obj} \sum_{c\in classes}^{} \begin{pmatrix} p_{i}(c) - \hat{p}_{i}(c)
\end{pmatrix}^{2} $$

- 크게 보면 bbox error + confidence score error + classify error
- x : truth 값, x^ : predict 값
- (I_ij)^obj : i번째 그리드 셀의 j번째 bbox에 object 있으면 1 / 없으면 0


<br>


## Inference 

Inference도 training과 마찬가지로 하나의 네트워크만 통과하면 된다.
Pascal VOC 기준 7x7 그리드 셀로 나누고, 그리드 셀 하나 당 2개의 bounding box를 생성한다. 즉, 총 98(7x7x2) 개의 bbox에 대한 prediction이 일어난다.


<br>

## Limitation of YOLO

앞에서 한 번씩 언급했지만 YOLO의 한계는 다음과 같다.
- YOLO는 하나의 그리드 셀 당 2개의 bbox, 한 종류의 class만 예측할 수 있다.
    - 따라서 새때와 같이 하나의 그리드 셀 안에 복수의 작은 객체들이 존재하거나, 다른 종류의 객체가 존재하면 이를 탐지하지 못한다.
- bbox를 생성하기 전 여러 번의 down sampling을 거치기 때문에 bbox가 정밀하지 못하다.
- 마지막으로 큰 bbox와 작은 bbox가 같은 가중치를 갖는다.


<br>


# 🚀 Comparison to Other Detection Systems.

## vs DPM (Deformable part model)

- DPM : extract static features → classify regions → prdict bbox
- Sliding window 방식
- (static feature가 뭔가 찾아봤더니 histogram을 분석하는 방식으로 feature를 독특하게 뽑아낸다. 근데 cnn에 비해 너무 복잡해 보임)

<br>

## vs R-CNN 계열

- Region proposal 기반
    - 300 ~ 2000 개의 영역을 제안하는 rcnn과 달리 yolo는 단 98개의 bbox만을 생성한다. 
    - 따라서 같은 object를 여러 개의 bbox가 예측해버리는 현상이 훨씬 덜 발생한다.
- 굉장히 복잡한 pipeline에서 rpn + fast rcnn 구조까지 발전했다.
    - 그러나 여전히 두 네트워크가 독립적으로 tuning
    - yolo는 한 방에 optimize 가능


<br>


# 🚀 Experiments

## Comparison to Other Real-Time Systmes

![mAP](https://user-images.githubusercontent.com/96368476/195112524-56076974-bccd-4b05-b21c-48677d35f83e.png){: width="50%" height="60%" .align-center}

- Real-Time 모델들과 비교하면 mAP 압도적
- Less than real-time으로 가면 더 높은 mAP 모델들 존재
- Backbone network를 VGG16으로 바꾸니 mAP 조금 증가

<br>

## VOC Error Analysis

Error의 종류를 분류해보는 재미있는 내용도 있다. YOLO의 경우 Fast R-CNN에 비해 localize error에 취약한 모습을 보이고, 반대로 Fast R-CNN은 background 탐지에 약한 모습을 보인다. 원인으로는 Fast RCNN : 제안된 region만 학습 / YOLO : 이미지 전체를 학습하기 때문인 듯하다.

![error](https://user-images.githubusercontent.com/96368476/195113357-430f5580-9891-433d-ba11-d91ba56680c7.png){: width="40%" height="50%" .align-center}

- Correct : correct class & IoU > 0.5
- Localization : correct class & 0.1 < IoU < 0.5
- Similar : class is similar & IoU > 0.1
- Other : class is wrong & IoU > 0.1
- Background : IoU < 0.1 for any class (include background class)

<br>

## VOC Results

![results](https://user-images.githubusercontent.com/96368476/195115529-5b10ffc7-3e5c-45eb-a8fe-726be29c57f8.png){: .align-center}

Less than real-time 모델들을 포함한 leaderboard. YOLO의 mAP를 클래스별로 살펴보면 bird, sheep 같이 작고 뭉쳐있는 객체에서 성능이 크게 떨어지는 것을 알 수 있다.


<br>

## Generalizability : Person Detection in Artwork

일반적으로 academic dataset은 testing daset과 유사한 분포를 가진다. Academic dataset으로 훈련시킨 모델이 현실의 dataset에서도 잘 작동할까? 다시 말해 학습 과정에서 보지 못한 데이터를 보아도 유연하게 대응이 가능할까? 이를 알아보기 위해 VOC로 학습한 모델들을 모아 피카소나 고흐 등 artwork를 탐지하게 해봤다.

![artwork](https://user-images.githubusercontent.com/96368476/195118655-b6505733-3769-41e2-9f4f-7f5bb9f711ca.png){: .align-center}

- YOLO 우승!
- 이 역시 이미지 전체를 학습하는 특징 때문일 것이라 생각


<br>


# 🚀 Code

> 추가 예정




<br>
<br>



[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}