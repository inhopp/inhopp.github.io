---
title:  "[논문 리뷰] You Only Look Once: Unified, Real-Time Object Detection (YOLO)" 

categories:
  -  Paper
tags:
  - [ML, Object Detection]

toc: true
toc_sticky: true

date: 2022-10-11
last_modified_at: 2022-10-11
---

**Paper: <br>- [You Only Look Once: Unified, Real-Time Object Detection (YOLO)](https://github.com/inhopp/inhopp/files/9751618/yolo_v1.pdf)**
{: .notice--primary}


<br>

# 🚀 Abstract

![test](https://user-images.githubusercontent.com/96368476/195012907-6d0ff077-7692-4929-bb14-bdd658a197cd.png){: width="80%" height="90%" .align-center}

지금까지 Detection 작업은 Multi-task problem이었다. Faster R-CNN을 예로 들어보자. 먼저 Region Proposal Networks를 통해 object가 있을 법한 영역을 제안하고 그 위에서 bbox regression & classify 작업을 진행했다. 더욱이 모델 훈련에서는 두 네트워크를 분리하여 fine tuning을 진행해야 하는 등 전체 모델이 복잡한 pipeline을 구성한다. 오늘 소개하는 YOLO 모델은 복잡한 multi-task problem을 하나의 regression problem으로 만들었다. 즉, 기존의 방식처럼 rpn이 제안하는 영역을 기다렸다가 수많은 영역들을 보아야 하는 것이 아니라 전체 이미지를 한 번 보는 것만으로 충분하다는 것이다. 따라서 모델의 inference speed가 많이 빨라졌는데 기본적으로 45fps, 모델을 작게 만든 fast yolo의 경우 155fps를 달성했다. 분명 faster rcnn (5fps) 공부할 때 real-time 모델이라고 배웠는데, 본 논문에서는 5fps가 무슨 real time이냐며 급 나누기를 시전한다. CVPR 2016에서 저자가 직접 real-time detection을 시연하는 영상이 남아있는데 지금 봐도 굉장히 멋지다.


<br>


# 🚀 Introduction

### Faster RCNN Architecture
![architecture](https://user-images.githubusercontent.com/96368476/190898741-a73d296a-c5c4-4d28-95ed-11e09d5501e6.png){: width="70%" height="80%" .align-center}

기존의 Detection 모델, Faster RCNN은 먼저 region proposal을 통해 bounding box를 생성하고 생성된 box에 대해 classification 작업을 수행하는 multi-task 방식이다. 이런 복잡한 방식은 inference 속도가 느리고, 무엇보다 독립적인 네트워크가 따로 훈련하기 때문에 최적화가 매우 까다롭다.

<br>

### YOLO Architecutre

![architecture](https://user-images.githubusercontent.com/96368476/195017645-36272d54-36a3-4496-aa6f-adb0dd530b8d.png){: width="70%" height="80%" .align-center}

반면 YOLO 모델은 하나의 이미지 픽셀로부터 bbox, class porb를 예측하는 single regression problem 구조를  이룬다. 이처럼 원본 이미지 전체를 input으로 받으며, 바로 최적화가 가능하다는 등 여러 장점이 존재한다. YOLO의 장점을 좀 더 자세히 설명하면 다음과 같다.

- YOLO는 complex pipeline이 아닌 single regression problem 구조를 가지기 때문에 매우 빠르다.
    - Experiments 파트에 나오지만, 실질적인 real-time 모델 중 성능이 압도적이다.
- 제안된 특정 영역이 아닌 이미지 전체를 global하게 보고 classify 한다.
    - 즉 객체뿐만 아니라 주변 정보도 같이 학습한다는 의미이다.
    - Faster RCNN에 비해 back-ground error가 절반 이하로 줄어든다.
- YOLO는 조금 더 일반화된 표현을 학습할 수 있다.
    - 위와 비슷한 맥락으로 주변 정보도 함께 학습하기 때문에, 학습 과정에 등장하지 않은 데이터가 들어와도 유연하게 대응한다.


하지만 YOLO도 완벽하지는 않다. 뒤에서 살펴보겠지만 구조적인 문제로 작은 객체를 탐지하는 데 어려움을 겪는다. 또한 속도에 큰 비중을 두어서 다른 detection model들에 비해 mAP가 떨어지는 편이다.



<br>


# 🚀 Model Architecture

![conv](https://user-images.githubusercontent.com/96368476/195030890-d3c157ee-b991-4d49-8963-d24000b00e5c.png){: width="60%" height="70%" .align-center}

YOLO의 전체 pipeline을 간단하게 살펴보자. 우선 이미지가 들어오면 feature extract를 담당하는 conv layer를 거친 후 SxS 그리드로 나눈다. 사용한 conv layer는 GoogleNet 모델의 inception 모듈을 사용했다. 다만 inception module을 (1x1 reduction + 3x3 conv) 로 단순화했다.

<br>

![bbox](https://user-images.githubusercontent.com/96368476/195029804-687a33fa-e946-439d-b091-257c32bba84f.png){: width="60%" height="70%" .align-center}

$$ \textbf{C} = \textbf{Pr}\left ( \textbf{Object} \right ) * \textbf{IoU}_{pred}^{truth} $$

이제 전체 이미지 사이즈가 SxS 그리드로 바뀌었다. 각 그리드 셀은 B 개의 bounding box를 예측하는데 하나의 box당 5개의 성분이 존재한다. Bounding box의 위치를 결정하는 w,h,x,y 값들, 그리고 confidence score 이다. Confidence score는 box 안에 객체가 존재하는지, box 모양이 얼마나 정확한지를 판단하는 점수이다. 만약 해당 bbox 안에 객체가 없다면 confidence score의 값은 0이 되어야 한다. 참고로 ground truth bbox는 객체의 중심에 해당하는 그리드 셀에서 생성한 bbox이다.


<br>

![output_tensor](https://user-images.githubusercontent.com/96368476/195035768-8538b583-9f54-4b87-b809-766ea0716039.png)

다음으로 각 그리드 셀은 bbox뿐만 아니라 각 class에 대한 probability도 존재한다. Class별 확률은 conditional probability이며, 주의할 점은 bbox별 class prob가 아니라 그리드 셀에 해당하는 class prob이라는 것이다. 여기에서 YOLO의 문제가 발생한다. 각 그리드 셀 당 하나의 class만을 예측하므로 만약 하나의 그리드 셀 안에 복수의 object가 포함되어 있다면 이를 탐지하지 못한다.



<br>
<br>



[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}