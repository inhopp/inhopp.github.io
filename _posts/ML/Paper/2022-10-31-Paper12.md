---
title:  "[논문 리뷰] YOLOv3 : An Incremental Improvement" 

categories:
  -  Paper
tags:
  - [ML, Object Detection]

toc: true
toc_sticky: true

date: 2022-10-31
last_modified_at: 2022-10-31
---

**Paper: <br>- [YOLOv3 : An Incremental Improvement](https://github.com/inhopp/inhopp/files/9896341/yolo_v3.pdf) <br>Reference: <br>- [PR-207: YOLOv3: An Incremental Improvement](https://www.youtube.com/watch?v=HMgcvgRrDcA)**
{: .notice--primary}


<br>

# 🚀 Introduction

![result](https://user-images.githubusercontent.com/96368476/198892154-8928dc6f-720a-4da0-8683-ae36178799ca.png){: width="40%" height="50%" .align-center}

저자는 지난 1년 동안 트위터와, GAN을 가지고 놀며 많은 시간을 보냈다고 한다. 또한 아주 흥미로운 내용이랄 것이 없고, 기존의 yolo 모델을 개선시킨 정도라고 솔직하게 말한다. 논문을 읽어보면 블로그 글을 읽는다는 느낌이 들고, 구성도 조금 특이해서 내가 편한 대로 재구성했다. YOLO v2 논문도 포스팅하려 했는데, 모델의 변화는 없고 전/후처리만 개선한 정도라 생략했다. YOLOv2에 등장한 테크닉들도 본 포스팅에 포함할 예정이다. 아무튼 결과만 미리 말하자면 yolo v2이후에 등장한 detection 모델인 SSD, RetinaNet과 비슷한 mAP를 달성하면서도 약 3배 정도 빠르다고 한다.


<br>


# 🚀 Model Architecture

![architecture](https://user-images.githubusercontent.com/96368476/198893210-a077be70-2105-4bbf-a6ae-d75e5a539a6a.png){: width="40%" height="50%" .align-center}

YOLOv3의 가장 큰 특징은 prediction을 3번 진행한다는 것이다. 기존 yolo모델은 이미지 전체에 대해서 총 98개의 bbox만을 예측한다. 따라서 inference 속도는 빠르지만, bbox의 정확도가 많이 떨어진다는 한계가 있었다. YOLOv3에서는 Feature Pyramid Network(FPN)의 아이디어를 도입했다. 보통 CNN에서 conv + pooling layer를 거치면 더 높은 차원의 feature를 뽑아낼 수 있지만 동시에 위치 정보에 대한 손실이 발생한다. FPN의 핵심 아이디어는 여러 번의 conv + pooling layer 이후 생성된 최종 output(고차원의 feature)도 이용하고, 위치 정보를 덜 손실한 이전 layer들의 output들도 이용하는 것이다. YOLOv3에서는 feature extractor를 거친 후 생성된 19x19 image에서 prediction을 수행, (upsample + feature extractor의 이전 layer ouput) 38x38에서 다시 prediction을 수행한다. 이런 방식으로 총 3번의 prediction을 수행하게 된다.







<br>
<br>



[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}