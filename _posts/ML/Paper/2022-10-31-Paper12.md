---
title:  "[Paper Review] YOLOv3 : An Incremental Improvement" 

categories:
  -  Paper
tags:
  - [ML, Object Detection]

toc: true
toc_sticky: true

date: 2022-10-31
last_modified_at: 2022-10-31
---

**Paper: <br>- [YOLOv3 : An Incremental Improvement](https://github.com/inhopp/inhopp/files/9896341/yolo_v3.pdf) <br>Reference: <br>- [PR-207: YOLOv3: An Incremental Improvement](https://www.youtube.com/watch?v=HMgcvgRrDcA)**
{: .notice--primary}


<br>

# 🚀 Introduction

![result](https://user-images.githubusercontent.com/96368476/198892154-8928dc6f-720a-4da0-8683-ae36178799ca.png){: width="70%" height="80%" .align-center}

저자는 지난 1년 동안 트위터와, GAN을 가지고 놀며 많은 시간을 보냈다고 한다. 또한 아주 흥미로운 내용이랄 것이 없고, 기존의 yolo 모델을 개선시킨 정도라고 솔직하게 말한다. 논문을 읽어보면 블로그 글을 읽는다는 느낌이 들고, 구성도 조금 특이해서 내가 편한 대로 재구성했다. YOLO v2 논문도 포스팅하려 했는데, 모델의 변화는 없고 전/후처리만 개선한 정도라 생략했다. YOLOv2에 등장한 테크닉들도 본 포스팅에 포함할 예정이다. 아무튼 결과만 미리 말하자면 yolo v2이후에 등장한 detection 모델인 SSD, RetinaNet과 비슷한 mAP를 달성하면서도 약 3배 정도 빠르다고 한다.


<br>


# 🚀 Model Architecture 

![architecture](https://user-images.githubusercontent.com/96368476/198893210-a077be70-2105-4bbf-a6ae-d75e5a539a6a.png){: .align-center}

YOLOv3의 가장 큰 특징은 prediction을 3번 진행한다는 것이다. 기존 yolo모델은 이미지 전체에 대해서 총 98개의 bbox만을 예측한다. 따라서 inference 속도는 빠르지만, bbox의 정확도가 많이 떨어진다는 한계가 있었다. YOLOv3에서는 Feature Pyramid Network(FPN)의 아이디어를 도입했다. 보통 CNN에서 conv + pooling layer를 거치면 더 높은 차원의 feature를 뽑아낼 수 있지만 동시에 위치 정보에 대한 손실이 발생한다. FPN의 핵심 아이디어는 여러 번의 conv + pooling layer 이후 생성된 최종 output(고차원의 feature)도 이용하고, 위치 정보를 덜 손실한 이전 layer들의 output들도 이용하는 것이다. YOLOv3에서는 feature extractor를 거친 후 생성된 19x19 image에서 prediction을 수행, (upsample + feature extractor의 이전 layer ouput) 38x38에서 다시 prediction을 수행한다. 이런 방식으로 총 3번의 prediction을 수행하게 된다.

<br>


![output](https://user-images.githubusercontent.com/96368476/198977572-788476a6-187e-4eb3-bd38-23458060ffed.png){: width="30%" height="40%" .align-center}

Prediction을 수행할 때 각 grid cell마다 3개의 bbox를 예측하게 된다. YOLOv1에서 총 98개의 bbox에 대해 예측을 했던 반면, YOLOv3에서는 총 10647개의 bbox에 대한 예측을 수행한다. 기존의 YOLO 모델은 빠르지만 적은 수의 bbox에 대한 예측으로 recall 성능이 많이 떨어진다는 단점이 있었는데, version3 에서 이 문제를 해결한 것이다. 참고로 Objectness score < 0.5인 경우 즉, bbox 내 객체가 없다고 판단할 경우 bbox coordinates와 class score는 training하지 않는다.


<br>

<br>


# 🚀 Feature Extractor

> YOLOv3에서 backbone으로 사용한 Darknet-53 <br>Layer 갯수도 훨씬 많아지고, Residual connection이 추가되었다.

![darknet1](https://user-images.githubusercontent.com/96368476/199037366-c797e8ac-247b-4cd0-8983-ad5dfd844254.png){: width="60%" height="70%" .align-center}

<br>

![darknet2](https://user-images.githubusercontent.com/96368476/199037371-af988f27-ce71-4f45-a518-e08193207f10.png){: width="60%" height="70%" .align-center}

기존 모델에서 backbone으로 사용한 Darknet-19의 경우 171 FPS 이다. 171 FPS는 과하게 빠르니 속도를 조금 늦추더라도 mAP 성능을 올리자는 것이 Darknet-53의 컨셉이다. 여기서 흥미로운 지점은 BFLOPS/s 이다. ResNet의 경우 최종 FPS는 Darknet보다 빠르지만 초당 연산량은 DarkNet이 압도적으로 높다. 페이퍼에는 GPU를 효율적으로 사용했기 때문이라고만 나오는데, 내가 참고한 유튜브 영상에서는 layer의 갯수 때문일 것이라 추측한다. Layer의 갯수가 많으니 cpu에서 gpu로 넘어가는 overhead가 발생한다는 것이다. (이 부분은 나도 확신이 없다. 데이터가 layer별로 올라가나? 코드에서는 모델 전체를 통으로 올리는데.. 나중에 찾아보자)


<br>



# 🚀 Anchor Box Prediction

## Dimension Clustering

![bbox_clustering](https://user-images.githubusercontent.com/96368476/198940735-4711a249-89a3-4bd6-a4f3-b0a50ba0eee8.png){: width="50%" height="60%" .align-center}

Bounding boxes with dimension clustering은 YOLOv2에서 도입된 기법이다. 해당 기법은 bounding box의 초기 세팅값을 ground truth 값의 k-means clustering으로 정하는 것이다. 본 논문에서 COCO dataset을 사용해 구한 9개의 cluster값은 다음과 같다.

- 첫 번째 prediction에서 사용 : (10x13), (16x30), (33x23)
- 두 번째 prediction에서 사용 : (30×61), (62×45), (59× 119)
- 세 번째 prediction에서 사용 : (116 × 90), (156 × 198), (373 × 326)


<br>

## Bounding Boxes Location

![bbox](https://user-images.githubusercontent.com/96368476/198985255-592f9677-67d9-42ae-81dd-50990c010b44.png){: width="50%" height="60%" .align-center}

YOLOv3에서는 bbox의 좌표도 기존과 달라졌다. 우선 bbox의 중심 (x,y)좌표는 grid cell의 모서리 지점 (c_x, c_y)를 offset으로 둔다. 또한 loss function에서 구한 t_x, t_y 값에 시그모이드를 취해 더해준다. 즉, offset + 0~1 value 로 새로운 중점이 grid cell을 벗어나지 못한다. 이러한 새 좌표 시스템이 성능에 유의미한 영향을 미쳤다고 한다.


<br>



# 🚀 Class Prediction

먼저 기존 모델과의 가장 큰 차이점은 softmax가 아닌 multi-label classification을 수행했다는 것이다. 저자들이 사용한 COCO dataset의 경우 90개의 class가 존재한다. 이때 Person / Woman 같이 계층적인 별개의 클래스가 존재하기 때문에 softmax가 적합하지 않다고 판단했다. 또 하나의 중요한 개선은 bbox별로 classify를 진행한다는 것이다. YOLOv1의 경우 bbox단위가 아닌 grid cell 단위로 하나의 class를 예측하였다. 


<br>



# 🚀 About COCO Metric

![benchmark](https://user-images.githubusercontent.com/96368476/199054729-f2661076-f6eb-4da4-860a-0d665ae4a721.png){: .align-center}


COCO dataset에서 사용하는 metric은 조금 특이하다. IoU threshold를 0.5 ~ 0.95까지 0.05씩 증가시켜가며 구한 mAP를 평균하는 방식이다. 이런 방식으로 구한 AP에서 YOLOv3의 성능은 많이 떨어진다. 하지만 AP_50에서는 괜찮은 성능을 보인다. 저자는 이러한 metric 방식이 합리적이지 않다고 한다. 실제로 COCO 논문에서 사람들을 대상으로 IoU 0.3과 0.5를 구분하라고 시켰더니 잘 못했다고 한다. 다시 말해서 AP 성능은 중요하지 않다는 것이다. 기존의 평가 방식에서 좋은 성능을 얻지 못했을 때, '채점 기준이 틀렸어 내 모델은 좋은 모델이야' 하는 모습이 굉장히 멋져 보인다.


<br>


# 🚀 Code

**[YOLOv3 from scratch (pytorch)](https://github.com/inhopp/YOLOv3)**
{: .notice--primary}



<br>
<br>



[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}