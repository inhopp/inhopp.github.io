---
title:  "[논문 리뷰] R-CNN 배경지식" 

categories:
  -  Paper
tags:
  - [ML, Paper]

toc: true
toc_sticky: true

date: 2022-02-17
last_modified_at: 2022-02-17
---

**Main Reference: <br>- [Deep Learning for Computer Vision](https://www.youtube.com/watch?v=dJYGatp4SvA&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r)** <br> 중간에 등장하는 ppt는 강의자료를 캡처한 것입니다.
{: .notice--warning}



<br>

# 🚀 Introduction

![1](https://user-images.githubusercontent.com/96368476/154297787-d69ddfd6-6e34-4b5e-a361-af5149b909bf.png){: width="60%" height="70%" .align-center}

이번에 소개할 R-CNN 논문은 딥러닝을 이용한 최초의 Object Detection 알고리즘이라 할 수 있다. 물론 비슷한 시도는 있었지만, 완성도 측면에서 첫 시도라 할 수 있겠다. 최근 사용되는 Object Detection & Semantic Sementation 기법을 이해하기 위해서 R-CNN의 내용은 필수적으로 알아야 한다.


<br>

# 🚀 Object Detection은 왜 어려울까?

R-CNN version1이 출시된 2013년의 상황을 상상해보자. 2012년 딥러닝을 이용한 image classification 모델인 AlexNet이 등장하면서 사람들은 image classification에 대해서는 자신감을 얻었을 것이다. 그 후 자연스럽게 classification의 다음 단계인 object detection 분야에 눈독 들였을 것이다. 하지만 조금만 생각해봐도 object detection 모델은 훨씬 까다롭다는 것을 알 수 있다.
<br>

![2](https://user-images.githubusercontent.com/96368476/154305212-c72d9dd9-c60d-4936-b694-501b8970d4a5.png){: width="50%" height="60%" .align-center}

- **Input** : 하나의 RGB Image
- **Multiple Output** : 몇 개의 object가 존재하는 지는 이미지마다 다르다
- **Multiple types of output** : 하나의 object마다 어떤 카테고리에 속하는지(what), 어디에 있는지(where) 즉, 다른 타입의 output이 필요하다.
- **Large Image** : object detection은 기본적으로 높은 해상도가 요구된다.

<br>

## Sliding Window

![3](https://user-images.githubusercontent.com/96368476/154308021-72c2d46a-b8b8-40ed-a523-07fc51cfce59.png){: .align-center}

가장 먼저 떠오르는 방법은 고정시킨 bounding box를 sliding 하면서 detection 하는 방법일 것이다. 하지만 가능한 bounding box의 모양, bouding box의 위치를 고려하면 눈앞이 캄캄해진다. R-CNN 에서 대안으로 사용한 방법은 후보 영역들을 제안해주는 Region Proposal 방법이다. 여담으로 R-CNN이 나오기 직전 가장 성능이 좋은 **Overfeat Detection** 모델은 sliding window를 이용한 방법이다. 논문에서 우리가 만든 R-CNN이 overfeat 보다 좋다고 계속 부관참시하더라.


<br>

# 🚀 Region Proposal

![4](https://user-images.githubusercontent.com/96368476/154305238-3d65a001-2c85-4004-8179-22f66e808796.png){: .align-center}

Region Proposal은 object가 있을 만한 위치 후보군을 제시해주는 모델이다. 이는 기존의 Computer Vision 분야에서도 논문이 우수수 쏟아지는 뜨거운 주제였는데 생태계 파괴 종 딥러닝의 등장 이후 모든 알고리즘은 씹어 먹혔다고 한다. (딥러닝 엔딩..) 아무튼 본 논문에서 사용한 Region Proposal 알고리즘인 Selective Search에 대해 알아보자.

<br>

## Selective Search

**상상 속 selective search**

![5](https://user-images.githubusercontent.com/96368476/154314768-a47129e9-3508-4165-b842-55d8ffc3e349.png){: width="80%" height="90%" .align-center}

Selective Search의 골자는 Color, Texture, Size, Fill 이렇게 4가지 구성 요소로 주변 영역들과의 Similarity를 계산한다. 그 후 similarity가 비슷한 영역들을 merge해 나가면서 최후에 남는 영역들을 제안해주는 알고리즘이다. 하지만 최후에 남는 영역들을 제안받으면 놓치는 object 들이 너무 많아서 본 논문에서는 2천 개의 후보 영역을 제안받는다.

<br>

**현실**

![6](https://user-images.githubusercontent.com/96368476/154314775-135b9caf-5a10-45d4-8644-a255c56f4cda.png){: width="70%" height="80%" .align-center}

그래도 2천 개의 영역을 cpu로 몇 초안에 계산해 내는 훌륭한 알고리즘이다. Selective Search의 가장 큰 문제는 같은 object를 몇 픽셀 차이로 여러 번 제안한다는 것이다. 따라서 중복된 box를 제거하는 방법이 필요한데 이 때 사용되는 개념이 IoU, NMS 이다.


<br>

## IoU - Intersection over Union

![7](https://user-images.githubusercontent.com/96368476/154318915-63b70d79-701a-4dc7-ae97-a370fc4f2668.png){: width="50%" height="60%" .align-center}

우리의 전략은 실제 bounding box인 Ground-Truth box와 가장 가까운 box를 빼고 전부 치워버리는 것이다. 그려려면 두 box가 서로 가깝다는 것이 어떤 의미인지를 정해야 하는데 그 때 사용하는 기준이 바로 **IoU** 이다.

<br>

![8](https://user-images.githubusercontent.com/96368476/154318916-1c716636-247b-4746-a078-d765c9313b18.png){: .align-center}

- IoU > 0.5 : 음 그럭저럭 잘 맞네
- IoU > 0.7 : 아주 좋아
- IoU > 0.9 : 완벽해! (실제로는 pixel 몇 개 차이)

이제 box-metric을 정의했으니, Non-Max Suppression(NMS) 알고리즘을 이해할 수 있다. NMS는 말 그대로 max값 빼고 전부 압축시키겠다는 의미이다.

<br>

## NMS - Non Max Suppression

![9](https://user-images.githubusercontent.com/96368476/154318918-d9c90def-0634-4041-88a0-1a85cbe88cf4.png){: width="50%" height="60%" .align-center}

1. 가장 점수가 높은 box 선택
2. IoU > threshold (ex 0.7) 인 box들 전부 제거
   - threshold 값은 hyper parameter
3. 다시 1번으로 (남는 box가 없을 때까지)

<br>

![10](https://user-images.githubusercontent.com/96368476/154318900-472a718d-aef7-4711-a590-1f486b2fe073.png){: .align-center}


<br>

## NMS 한계

![11](https://user-images.githubusercontent.com/96368476/154318912-e8697b16-3e19-4595-9e0d-23198cfb3de8.png){: width="50%" height="60%" .align-center}

- NMS는 Object Detection 분야에서 반드시 필요한 좋은 알고리즘이다.
- 하지만 object들이 highly overlapping되면 **<span style="color:red">실제 필요한 box들도 지우게 된다.</span>**
- 이는 Object Detection 분야의 Open Problem 이라고 한다.




<br>

# 🚀 Evaluating Object Detection

> 두 Detector를 단순히 비교하기에는 조금 애매한 측면이 있다. <br>Object Detection의 평가 지표인 mAP에 대해 알아보자.


![14](https://user-images.githubusercontent.com/96368476/154418727-105df134-8604-47db-87fa-dd08bd7d251c.png){: .align-center}

- 왼쪽 Detector : 틀린 Detecting 있지만, <u>모든 고양이 Detect</u>
- 오른쪽 Detector : 모든 고양이를 Detect하지 못했지만, <u>틀린 Detecting 없음</u>
- 어떤 것이 더 좋은 Detetor일까?


<br>

## Precision(정확도) vs Recall(재현율)

$$Pricision = \frac{예측 성공한 영역 갯수}{Detector가 예측한 영역 갯수}$$

$$Recall = \frac{예측 성공한 영역 갯수}{실제 Object 영역 갯수}$$

![15](https://user-images.githubusercontent.com/96368476/154420333-4539dcc1-2db2-477d-b2fd-1bf447ca8b9c.png){: width="50%" height="60%" .align-center}

<br>



## mAP - mean Average Pricision

![16](https://user-images.githubusercontent.com/96368476/154421095-ec8877ab-e175-4c55-81d8-976061e2d6ae.png){: .align-center}

- 우선 Prediction Score로 box들을 sorting 한다.
  - 1번 box: 고양이일 확률 가장 높음
  - 2번 box: 고양이일 확률 두 번째로 높음 
  - ...
- Prediction box(빨간색)와 Ground-Truth box(파란색)의 IoU가 0.5 이상이면 예측 성공
- IoU > 0.5 → 예측 성공 / IoU < 0.5 → 예측 실패
- 이를 기준으로 PR Curve를 그린다 (Pricision-Recall Curve)

<br>

![17](https://user-images.githubusercontent.com/96368476/154421114-3a68e782-d993-4cd4-9180-7cec5c306cfa.png){: width="80%" height="90%" .align-center}
![18](https://user-images.githubusercontent.com/96368476/154421124-0d011ff5-043d-4f77-823e-2c575b3f335a.png){: width="80%" height="90%" .align-center}

- 이때 PR-Cureve의 면적을 Average Pricision(AP)라 한다.
- 그럼 Category별로 AP를 구할 수 있을 것이다.
  - Dog AP = 0.7
  - Cat AP = 0.8
  - Car AP = 0.7
  - ...
- **mean Average Pricision (mAP) = AP들의 평균**

<br>

mAP는 지금까지 Detector의 평가 기준으로 사용된다. 참고로 R-CNN의 mAP = 53.3


<br>
<br>

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}