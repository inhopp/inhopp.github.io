---
title:  "[논문 리뷰] Rich feature hierarchies for accurate object detection and semantic segmentation (R-CNN)" 

categories:
  -  Paper
tags:
  - [ML, Paper, Object Detection]

toc: true
toc_sticky: true

date: 2022-02-18
last_modified_at: 2022-02-18
---

**Main Reference: <br>- [Deep Learning for Computer Vision](https://www.youtube.com/watch?v=dJYGatp4SvA&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r) <br>- [Life Time Study](https://www.youtube.com/watch?v=VYILt52ETqw)**
{: .notice--warning}


**Paper: <br>- [Rich feature hierarchies for accurate object detection and semantic segmentation](https://github.com/inhopp/inhopp/files/8081058/RCNN.paper.pdf)**
{: .notice--primary}


**[Object Detection 배경지식](https://inhopp.github.io/paper/Paper2/) : 본 논문을 이해하는데 필요한 배경지식을 정리한 포스팅 입니다.**
{: .notice--success}



<br>

# 🚀 Abstract

![2](https://user-images.githubusercontent.com/96368476/154619532-c342913a-181a-44bd-b430-89cea695fabd.png){: width="70%" height="80%" .align-center}

(R-CNN 논문이 처음 나온 2013년 기준) 최근 몇 년 Object Detection의 성능은 정체기에 들어섰다. 가장 성능이 좋은 모델은 기존의 복잡한 모델들을 앙상블 한 것이다. 우리가 제안하는 모델은 기존 최고 성능의 모델보다 30% 향상된 53.3% mAP를 달성했다. 뿐만 아니라 비교적 간단하고, data set이 달라져도 확장이 쉽다. 우리 모델의 key point는 다음 두 가지이다.
- Detection 분야에 CNN 접목
- annotated data가 부족한 상황에서 기존의 패러다임을 전환



<br>

# 🚀 Introduction

**샤라웃 CNN!** Intro에서 저자들은 CNN의 발전 과정, Image Classification 분야의 성공을 짚어준다. 그러면서 CNN의 등장이 Detection 분야에 활력을 넣어주었다고 말한다. CNN을 어떻게 확장시켜야 detection에 접목할 수 있는지 많은 논의가 이루어졌고, 본 논문은 처음으로 CNN을 활용하여 드라마틱한 성능 향상을 이끌어냈다.
<br>

![4](https://user-images.githubusercontent.com/96368476/154625195-e94406e1-1f1b-460a-89de-ac98be55f37d.jpg){: width="50%" height="60%" .align-center}

우리의 주요 관심사는 다음 두 가지였다.
- Localizing object
- Small quantity of annotated detection data


<br>

## Localizing

Detection은 classify와 다르게 localizing이 필요하다. 우리는 localizing 방법으로 세 가지 접근을 고려했다.
- Localization as a regression problem
  - 논문 하나를 reference로 두면서 현실적이지 않다고 한다.
- Sliding-window detector
  - object detection의 Input은 high resolution Image이어야 함 (고해상도 이미지)
  - Image size가 크기 때문에 conv, pool 과정을 여러 번 해야하는데, 그러면 receptive field가 너무 커지는 문제 발생
  - ![5](https://user-images.githubusercontent.com/96368476/154627009-cf83638e-48b1-423f-94c7-e32ece00371d.png){: width="50%" height="60%"}<br>
- Recognition using regions
  - object가 존재할 만한 후보 영역을 제안해주는 region proposal 알고리즘 사용
  - region proposal & cnn 궁합이 아주 좋음
  - 본 논문 이전에 나온 overfeat 모델은 sliding-window 방식 (우리가 mAP 훨씬 높음)
  - 논문 제목 나두고 왜 R-CNN이라 불러요?
  - **<span style="color:red">"We call our method R-CNN : Regions with CNN"</span>**


<br>

## 부족한 annotated data

![1](https://user-images.githubusercontent.com/96368476/154297787-d69ddfd6-6e34-4b5e-a361-af5149b909bf.png){: width="60%" height="70%" .align-center}

하나의 사진. 수 많은 object들. 이 많은 data set을 누가 다 labeling 했을까.(대학원 가지 마?) <br>저자들은 training data가 부족할 때 전통적으로 해오던 방식과 다른 방식으로 접근했다고 한다.
- 기존의 모델들은 unsupervised pre-training → fine-tuning
- 하지만 우리는 패러다임을 전환했음
- classification에 사용된 data set으로 supervised pre-training
- 부족한 detecting data로 fine-tuning
- **<span style="color:red">즉, supervised pre-training → fine-tuning</span>**
- 이 방법을 사용하니 mAP가 8%나 향상되었다.


<br>

## 우리 모델의 장점
- CNN의 모든 parameters가 공유되기 때문에 기존 모델들 덕지덕지 붙인 앙상블보다 훨씬 가벼움
- Region-based라 segmatation하기도 좋음
- 간단한 box-regression 추가했더니 localizing도 아주 잘함



<br>

# 🚀 Model Architecture

![7](https://user-images.githubusercontent.com/96368476/154653268-df2674f8-5001-4802-89bc-b87184b04b67.jpg){: width="80%" height="90%" .align-center}
![8](https://user-images.githubusercontent.com/96368476/154653275-29ad07aa-a907-478a-b0b2-ac532baa4d0c.jpg){: width="80%" height="90%" .align-center}

R-CNN은 3가지 모듈로 구성되어 있다.
- Category-independent region proposals
   - 최근 Region proposal에 관한 논문이 많이 나오지만 우리는 selective search 알고리즘을 사용했다.
- Convolutional Neural Networks
  - Caffe Framework 사용 (버클리에서 만든 프레임워크, C++ 기반)
  - AlexNet 사용
- A set of class-specific linear SVMs
  - Region 별로 binary-classify
    - Dog? no
    - Cat? no
    - Person? yes
    - ...
<br>

## Appendix A : object proposal transformation

Selective Search 알고리즘으로 region proposal을 받으면 제안받은 영역의 모양은 제멋대로일 것이다. 이 영역들로 CNN을 돌리려면 227x227로 사이즈를 맞춰줘야 하는데 이 과정을 'object proposal transformation' 이라 한다. 우리는 여러 방법 중 가장 간단한 **Warping** 을 택했다. 다음 그림은 여러가지 transform에 대한 예시이다.

![10](https://user-images.githubusercontent.com/96368476/154661185-d077616f-6399-433a-8407-2fda8c8e4931.png){: width="70%" height="80%" .align-center}

- (A) : 제안받은 영역
- (B) : tightest square with context 
  - 원본 이미지에서 영역을 포함하는 가장 작은 정사각형 선택
  - 227x227로 resize
- (C) : (B)에서 배경 지우기
- (D) : Warping (영역 그대로 resize)

이때 그림의 아래 row는 테두리에 16 pixel씩 padding한 것이다. 패딩한 영역은 주변 pixel값의 평균으로 채웠다. Padding 추가시 mAP가 3-5% 증가하더라.



<br>

# 🚀 Training

1. ImageNet 대회에서 쓰던 data set(ILSVRC2012 Classification)으로 CNN <u>Supervised pre-training</u>
2. Annotated detection data로 <u>fine-tuning</u>
   - 이때 (N+1) 개의 Category Classification
   - N : number of object classes
   - +1 : backgorund
   - 즉, object가 아닌 영역이 들어오면 '이건 배경이야' 라고 training

여기서 특이한 점은 CNN으로 feature vertor를 뽑은 후에 linear SVM으로 classify 한다는 것이다. 저자들도 softmax 쓰면 더 깔끔해지는 거 알고 있다고 말한다. 그런데도 SVM을 사용한 이유는 Appendix B에 적혀있다.

<br>

## Appendix B : about positive & negative example

![12](https://user-images.githubusercontent.com/96368476/154704886-45d2c2e8-b52e-43ea-92d2-df8c845e8cc2.png){: width="80%" height="90%" .align-center}

기존의 전략은 IoU 0.5값을 기준으로 positive, negative를 구분하는 것이다. 그 후 positivie example은 해당 class로, negative example은 background class로 training 한다. 하지만 이렇게 하니 문제가 발생하더라.

- 클래스 불균형 (background region이 훨씬 많음)
- 애매하게 겹친 region들이 오히려 훈련을 방해 

### Hard negative example
<br>

![13](https://user-images.githubusercontent.com/96368476/154705469-a53c8d02-a201-4a88-a88b-6adb82f67ec7.png){: width="70%" height="80%" .align-center}

따라서 저자들은 IoU 0.5값을 기준으로 postive/negative를 나누지 않았다. 중간에 회색지대 (gray zone)를 만들고 이들을 훈련데이터에서 제외시켰다. softmax는 positive 아니면 무조건 negative이기 때문에 이러한 작업이 불가능하다.
- hard negative의 IoU 기준 값 0.3은 그리드 서치로 찾은 하이퍼 파라미터다.
- 이 값이 생각보다 중요하더라
  - 0.5로 설정하면 mAP 5% 감소
  - 0으로 설정하면 mAP 4% 감소


이렇게 뽑은 hard negative region 96개, positive region 32개 (총 128개) 로 minibatch 훈련시켰다. 결과적으로 mAP도 향상되고, 수렴 속도도 빨라졌다.



<br>

# 🚀 Bounding-box regression

우리는 이전 모델에서 영감을 받아 bounding box regression 모듈을 추가했다. 복잡한 수식으로 표현되어 있지만 내용은 다음과 같다.

![14](https://user-images.githubusercontent.com/96368476/154708578-0a7beec3-53d1-47ab-9f2b-65fe744f17ea.jpg){: width="80%" height="90%" .align-center}

- box는 x좌표, y좌표, 넓이, 높이로 주어짐
- selective search가 예측한 predict box
- x좌표, y좌표 : 평행이동 변환
- 넓이, 높이 : 원래 값 비례해서 변환
- 변환 후 box가 Ground-truth box (실제 box)와 같아지도록 training
- 변환 값들은 class 별로 4개씩 training

<br>

## bounding-box regression은 왜 필요할까?

> 개인적인 생각입니다. 논문에 나오는 내용은 아니니 참고해주세요.

![15](https://user-images.githubusercontent.com/96368476/154710253-9ac5a6c1-b124-44f3-8b35-142e46befe1c.png){: width="80%" height="90%" .align-center}

<br>

## Appendix C : about box regression

![16](https://user-images.githubusercontent.com/96368476/154716153-94353411-4c01-4768-8f09-05791ea7ac3f.png){: width="70%" height="80%" .align-center}

- box regression term + lasso 규제 term
- 람다 값 1000으로 규제 굉장히 강하게 함
  - 미세 조정 
- 단순한 positive box가 아닌 진짜 가까운 box들로만 훈련
  - IoU 0.6 이상
- box regression training하니 모델의 성능이 향상되었음
- 테스트 타임에서 box regression 반복은 의미 없었음. box 조정은 한 번만


<br>

# 🚀 Detection data set

본 논문에서 사용한 detection data set은 ILSVRC2013이다. 이 data set은 200가지 종류의 클래스들을 가지고 있고, train(395,918), val(20,121), test(40,152)개로 이루어져 있다. 근데 data set에 문제가 있었다.

- Training data의 annotation이 불완전하다
  - 이러면 object를 background로 인식해버리기 때문에 불완전한 train data 들은 모두 제거함
  - 결과적으로 training data가 부족하여 validation data로도 훈련을 진행함
- Input image의 size범위가 아주 작은 것부터 mega-pixel까지 매우 큼
  - region proposal에 영향을 주어서 width를 500으로 비율 맞추고 시작했음


<br>

## Data set에 따른 성능 비교

![17](https://user-images.githubusercontent.com/96368476/154723339-1a30c3c2-09d9-45bc-9d16-5141df8aa7b4.png){: width="80%" height="90%" .align-center}

1. training set 개수를 5000개 늘려주니 성능 향상
2. training set 5000개 더 추가했더니 영향 없었음
3. CNN fine-tuning 해주니 성능 향상
4. fine tuning 할 때 training set 개수 늘려주니 성능 향상
5. box regression하니 성능 향상





<br>

# 🚀 Conclusion

우리는 Computer Vision 분야에 Deep Learning 기술을 성공적으로 결합시켰다! 




<br>
<br>

[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}