---
title:  "[Paper Review] PGGAN : Progressive Growing of GANs for Improved Quality, Stability, and Variation" 

categories:
  -  Paper
tags:
  - [ML, Generative]

toc: true
toc_sticky: true

date: 2023-06-15
last_modified_at: 2023-06-15
---

**Main Reference: <br>- [PGGAN Paper](https://arxiv.org/abs/1710.10196)**
{: .notice--primary}

<br>


# 🚀 Introduction

최근 주목받고 있는 생성모델들의 장단점은 다음과 같다.

- Autoregressive model (ex. Pixel CNN) : produce sharp image / slow to evaluate / no latent space
- VAE : easy to train / produce blurry image
- GAN : produce sharp image / unstable training / low resolution & limited variation

특히 GAN의 경우 몇 가지 문제가 존재한다. 먼저, Generator는 KL-Divergence를 이용해 training distribution과 generated distribution을 같아지게 하는 것이 목적이다. 이때 두 분포의 겹치는 부분이 적다면(학습 초반) graidnet가 랜덤한 방향을 가리키게 된다. 최근에는 KL-Divergence 대신 least-square, absolute deviation, 또는 Wasserstein distance를 이용해 문제를 해결한다. 본 논문의 경우 Wasserstein loss를 조금 개량해서 사용하였다. 뿐만 아니라 GAN의 경우 High resolution 이미지를 생성하기가 어렵다. 그 이유는 high resolution으로 갈수록 training image와 generated image 사이의 거리가 멀어지기 때문이다. 이런 경우 gradient가 크게크게 움직이므로 안정적으로 학습하기가 어려워진다. 또한 resolution이 커지면 메모리 이슈때문에 batch size가 작아지고(stable한 학습을 위해서도 낮은 batch size가 유리), 학습이 오래 걸리게 된다. 본 논문의 경우 Generator와 Discriminator 모두 progressive하게 학습하여 학습 속도와 생성된 이미지의 퀄리티를 모두 향상시켰다.



<br>


# 🚀 Progressive Growing of GANs

![1](https://github.com/inhopp/inhopp/assets/96368476/6d4d90b7-c5e9-4ac7-8938-7b9e36ee5033){: width="70%" height="80%" .align-center}

PGGAN의 핵심 아이디어는 4x4 low resolution으로 시작해 high resolution 디테일들을 생성하는 layer들을 추가해주는 것이다. 이런 점진적인 방식은 동시에 모든 스케일을 학습하지 않고, 먼저 large-scale sturucture를 학습한 뒤 디테일들을 학습하게 된다. 


![2](https://github.com/inhopp/inhopp/assets/96368476/59422153-67d2-440a-871c-7fd49b3f4655){: width="70%" height="80%" .align-center}

이때 새로운 layer는 자연스러운 fade-in 방식으로 추가해준다. 위 방식은 기존의 잘 훈련된 row resolution layer가 갑자기 큰 충격을 받는 것을 방지해줌으로써 훈련을 안정적으로 할 수 있게 해준다. <br>이러한 progressive learining은 몇 가지 장점이 존재한다. 우선 초기의 row resolution 이미지에서는 클래스 정보다 모드의 갯수가 적기 때문에 이미지 생성이 안정적이다(4x4 크기의 이미지 분포는 뭉쳐져있음). 그 다음 resolution을 조금씩 증가시켜 디테일들을 추가해주는 방식이기 때문에, 기존 GAN처럼 latent에서 바로 가는 매핑보다 훨씬 안정적이다. 이러한 방식으로 1024x1024라는 전례 없는 해상도의 이미지를 생성할 수 있게 되었다. 뿐만 아니라 학습 속도도 2배~6배까지 단축되었다.


<br>



# 🚀 Increasing Variation Using Minibatch Standard Deviation











# 🚀 Code

> [PGGAN from scartch (pytorch)](https://github.com/inhopp/PGGAN)

|  |  |
|:-:| :-: |
|  |  |


<br>
<br>



[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}