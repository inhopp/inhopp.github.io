---
title:  "[논문 리뷰] Xception : Deep Learning with Depthwise Separable Convolutions" 

categories:
  -  Paper
tags:
  - [ML, Paper, Classification]

toc: true
toc_sticky: true

date: 2022-07-31
last_modified_at: 2022-07-31
---

**Paper: <br>- [Xception : Deep Learning with Depthwise Separable Convolutions](https://github.com/inhopp/inhopp/files/9228234/Xception.pdf)**
{: .notice--primary}


<br>

# 🚀 Abstract

| Simplified Inception | Reformulation Version | Extreme Version |
|:-:|:-:|:-:|
| ![1](https://user-images.githubusercontent.com/96368476/182015967-1b200631-b01d-4036-a08a-81ef77d0344e.png) | ![2](https://user-images.githubusercontent.com/96368476/182015968-15dc27e1-65ef-443b-ac67-964416fb0f93.png) | ![3](https://user-images.githubusercontent.com/96368476/182015969-6fc8d736-9406-404e-b444-cc2ca53300c6.png) |

저자들의 제시하는 inception module에 대한 해석은 simple convolution과 depthwise separable convolution의 중간 단계라는 것이다. 다시 말해 depthwise separable convolution module은 tower의 갯수가 매우 많은 inception module에 불과하다는 말이다(input channel을 얼마나 쪼갤 것인가의 차이). 이러한 관점에서 Inception V3의 다음 모델로 inception module을 depthwise separable module로 바꾼 모델을 생각할 수 있다. 저자들은 이 모델을 Extreme Inception, Xception 이라 이름 지었다. Xception은 Inception V3와 파라미터 갯수는 같으면서도 성능이 뛰어나다. 특히, 아주 큰 dataset에서는 그 차이가 더 벌어진다. 두 모델의 파라미터 갯수가 같기 때문에 성능의 차이는 모델의 크기에서 오는 것이 아닌, 얼마나 효율적으로 구성했느냐의 차이에서 오는 것이다. Depthwise Separable module이 왜 효율적으로 작동하는지 알아보자.


<br>


# 🚀 Introduction

## Inception Hypothesis

![6](https://user-images.githubusercontent.com/96368476/182018023-37a1691a-f1a8-4460-b434-b205e102d063.png){: width="50%" height="60%" .align-center}

Simple Convolution 연산을 생각해보자. convolution layer의 역할은 3D kernel로 channel별 feature, 2D spatial feature를 학습하는 것이다. 즉 하나의 커널이 spatial correlation, cross-channel correlation 두 가지 요인을 학습한다. **<span style="color:red">Inception module의 hypothesis(main idea)는 이 두 factor를 동시에 학습하는 것보다, 분리해서 학습하는 것이 더 효율적이라는 것이다.</span>** Inception은 먼저 cross-channel correlation을 구하고, 채널이 줄어든 공간에서 2d spatial correlation을 학습한다. 위에서 보았던 그림을 다시 보며 inception module의 철학을 살펴보자.

<br>


| Simplified Inception | Reformulation Version | Extreme Version |
|:-:|:-:|:-:|
| ![1](https://user-images.githubusercontent.com/96368476/182015967-1b200631-b01d-4036-a08a-81ef77d0344e.png) | ![2](https://user-images.githubusercontent.com/96368476/182015968-15dc27e1-65ef-443b-ac67-964416fb0f93.png) | ![3](https://user-images.githubusercontent.com/96368476/182015969-6fc8d736-9406-404e-b444-cc2ca53300c6.png) |

- 1by1 convolution을 따로 해야 할 필요가 있나? (simple -> reformulate)
- 1by1 convolution의 output channel을 가지고 2d spatial feature를 학습해야 하는데, 얼마나 많은 kernel이 필요하나?
- Extreme version : input channel의 갯수만큼 해주자 (reformulate -> extreme)

Extreme 버전 기저에 깔려있는 생각은 mush stronger hypothesis이다. 즉, "cross-channel correlation과 2d spatial correlation이 완전히 분리 가능하다"라는 가정인 것이다. 이 아이디어는 기존에 존재하던 depthwise separable convolution의 철학과 아주 유사하다. Inception의 철학을 따라가다 보면 depthwise separable이 자연스럽게 등장한다는 의미인데 굉장히 흥미로웠다. 둘 사이의 미묘한 차이점을 살펴보자.


<br>


## Extreme Inception vs Depthwise Separable

| Extreme Inception | Depthwise Separable |
|:-:|:-:|
| ![5](https://user-images.githubusercontent.com/96368476/182018022-e0e40e61-2f32-4458-b5d8-3ae0abf5bdee.png) | ![4](https://user-images.githubusercontent.com/96368476/182018020-3d6deb1f-822c-47ad-be54-e447998467d6.png) |

1️⃣ **연산의 순서가 다르다.**
- inception의 경우 1by1 conv로 cross-channel correlation을 구한 뒤에 depthwise convolution을 수행한다.
- depthwise separable의 경우 그 반대이다.

2️⃣ **첫 번째 연산 후 non-linearity (activation function)의 존재 여부**
- inception의 경우 각각의 연산 후 모두 ReLU 연산이 수행된다.
- depthwise separable의 경우 중간 단계에서는 activation function을 수행하지 않는다.

저자들은 1번 연산의 순서 차이가 stack setting에 불과할 뿐 중요하지 않다고 말한다. 나의 관점은 조금 다른데 마지막 dicussion에서 다루어 보자. 반면 2번 차이인 중간 non-linearlity의 존재 여부가 성능에 유의미한 영향을 끼침을 실험적으로 보여준다. 해당 실험의 내용은 Performance에서 다룬다.



<br>


# 🚀 Xception Architecture

![7](https://user-images.githubusercontent.com/96368476/182025972-aa183566-94ed-4ae2-9e05-d569f4610ade.png){: width="70%" height="80%" .align-center}

논문을 읽으면서 좀 어이가 없었던 부분인데, extreme inception 이야기 실컷 해놓고 막상 network는 depthwise separable block으로 만들었다(그림의 SeparableConv는 depthwise separable을 의미한다). Depthwise Separable block이 이미 tensorflow에 구현되어 있어서 만들고 유지보수하기 편해서라고 하는데 이것만으로는 납득이 잘 안 간다. 아마 기저에 깔린 철학이 xception과 같아서 혹은 모델의 motivation이 inception이라 그런듯하다. (혹은 google다니는 어른들의 사정?) <br> Xception architecture는 기본적으로 depthwise separable block의 linear stack으로 이루어져 있고, first & last module을 제외한 모든 module에 residual connection이 존재한다. 사소하지만 한가지 짚고 싶은 내용은 같은 block을 단순히 쌓은 형태의 구조이다. 논문에서는 VGG를 참고해서 만들었다고 하는데, 특별한 이유가 있는 것인지 자원의 한계 때문인지 궁금하다. MobileNet은 depthwise separable block 사이사이에 simple convolution이 들어있던데 기준을 모르겠다. 기준이란게 없을 수도 있고.



<br>


# 🚀 Performance

성능을 비교하기 전에 알아둘 점은 inception V3와 동일한 optimization configuration을 세팅했다는 것이다. 즉, xception에 더 최적화 할 수 있다는 말인데 직접 찾기는 귀찮으셨나 보다. 사용한 데이터셋은 ImageNet, JFT 두 종류이다.

| Training on ImageNet | Training on JFT |
|:-:|:-:|
| ![8](https://user-images.githubusercontent.com/96368476/182026499-663ce1a9-4be3-415c-9dcc-f3a5589ac9b4.png) | ![9](https://user-images.githubusercontent.com/96368476/182026500-64d16098-df9f-4688-a07c-879772669472.png) |

- ImageNet: 1000-class & single-label classification
- JFT: 17000-class & multi-class classification
- imagenet에서는 약간 더 좋고, JFT에서는 많이 좋다.
- (아마 inception v3가 imagenet을 기준으로 만들어서 그런듯)
- gradient step 속도는 inception이 조금 더 빠르다. (28/31 steps per sec)


<br>

## Effect of the residual connection

![10](https://user-images.githubusercontent.com/96368476/182026497-eb121420-4dbe-40a0-a8ad-55f32313e924.png){: width="50%" height="60%" .align-center}

Residual connection을 빼면 성능이 뚝 떨어지는 것을 볼 수 있다. Xception에서 residual connection은 speed와 accuracy를 동시에 올려주는 필수적인 요소이다. 그럼 residual connection은 무조건 좋을까? 그건 아니다. VGG의 base module을 depthwise separable로 바꾼 모델에서는 residual connection이 없을 때 성능이 좋았다고 한다. 즉, 모델의 구조에 따라 좋을 수도 나쁠 수도 있다는 의미이다.


<br>


## Effect of an intermediate activation

![11](https://user-images.githubusercontent.com/96368476/182026498-f0f9a346-6b7a-483c-b51a-9b59caf8b7a2.png){: width="50%" height="60%" .align-center}

Non-linearity가 오히려 성능을 떨어뜨린다는 것은 예상 밖의 결과이다. 더 주목할 지점은 같은 inception 모듈로 반대의 결과를 얻은 논문이 있다는 것이다. 해당 논문에 따르면 activation function이 deep kenel convolution(깊은 channel의 필터)에서는 도움이 되고, shallow kenel에서는 성능을 떨어뜨린다고 한다.


<br>

# 🚀 Future Direction

저자들은 inception module의 extreme version을 가지고 모델을 만들었지만, extreme버전이 반드시 optimal하다는 보장은 없다. 기존의 inception과 extreme inception 사이 중간 단계의 모듈들로도 충분히 모델을 만들 수 있고 혹시 더 좋은 성능을 나타낼지 모른다. (근데 너그들이 해봐라)



<br>

# 🚀 Discussion

## Extreme Inception vs Depthwise Separable

위 내용에서 저자들은 연산 순서 차이가 단지 stacking setting에 불과하다고 말한다. 하지만 직관적으로 생각해보면 정보를 압축시킨 후에 2d spatial feature를 뽑는 것보다 2d spatial feature를 먼저 뽑고 정보를 압축시키는 것이 더 합리적이지 않을까 싶다. 수학과 출신이라 그런지 교환법칙을 막 가져다 쓰면 어딘가 불편하다.. 나중에 여유가 생기면 검증해봐야겠다.



<br>


## Effect of the residual connection

내가 생각하는 residual connection의 의미는 다음과 같다. 정보를 강하게 압축했을 경우, 큰 feature들은 잘 뽑아내겠지만 작은 feature들은 놓치기 쉬워진다. 이때 놓치기 쉬운 작은 정보들을 보완해주기 위해 압축하기 전의 데이터를 더해주는 것이 residual connection이다. 즉, residual connection은 압축의 정도가 강할 수록 효과가 좋을 것이라는 추론이 가능하다. 혹시 VGG의 conv를 depthwise separable로 바꾼 모델에서는 channel-correlation을 얕게 압축한 것이 아닐까? 만약 그렇다면 이 부분은 충분히 설명이 가능하게 된다. 해당 모델의 구체적인 정보는 없어서 확인은 못해봤다.


<br>
<br>



[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}